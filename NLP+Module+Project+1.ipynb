{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cced9a6c",
   "metadata": {
    "id": "cced9a6c"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:2.4em;\"> AIML MODULE PROJECT - NLP Module Project 1 - Statistical NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff2010",
   "metadata": {
    "id": "e2ff2010"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#3A8D71;\"> SUBMITTED BY : RANJITHKUMAR S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167b2ee",
   "metadata": {
    "id": "8167b2ee"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:2.5em;color:#b44b97;\"> Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b783c",
   "metadata": {
    "id": "944b783c"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3A8D71;\">DOMAIN:</span> Digital content management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25557c1f",
   "metadata": {
    "id": "25557c1f"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3A8D71;\">CONTENT:</span>Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc.\n",
    "are written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f5296",
   "metadata": {
    "id": "2c2f5296"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3A8D71;\">DATA DESCRIPTION:</span> \n",
    "Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected posts of\n",
    "19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or\n",
    "approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger id# and\n",
    "the blogger’s self-provided gender, age, industry, and astrological sign. (All are labelled for gender and age but for many, industry and/or sign is\n",
    "marked as unknown.) All bloggers included in the corpus fall into one of three age groups:\n",
    "• 8240 \"10s\" blogs (ages 13-17),\n",
    "• 8086 \"20s\" blogs(ages 23-27) and\n",
    "• 2994 \"30s\" blogs (ages 33-47)\n",
    "• For each age group, there is an equal number of male and female bloggers. Each blog in the corpus includes at least 200 occurrences of\n",
    "common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the\n",
    "date of the following post and links within a post are denoted by the label url link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12276a",
   "metadata": {
    "id": "3c12276a"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3A8D71;\">PROJECT OBJECTIVE:</span> To build a NLP classifier which can use input text parameters to determine the label/s of the blog. Specific to this case\n",
    "study, you can consider the text of the blog: ‘text’ feature as independent variable and ‘topic’ as dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338e380d",
   "metadata": {
    "id": "338e380d"
   },
   "outputs": [],
   "source": [
    "#importing necessary python libraries\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#importing Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#importing langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "#import train test split model from sklearn model selection to create train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing the stopwords helps to remove the stopwords from the corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Helps to pad the sequences into the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Layers that are used to implement the LSTM model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "#importing label encode to convert the labels in the target columns into a number \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#importing countvectorizer from sklearn feature extracting for converting text data into numberic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#importing the MulitnomialNB function from sklearn naive bayes for the classification\n",
    "#importing the performance metrics from sklearn metrics \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,roc_auc_score\n",
    "\n",
    "#import TF-IDF Vectorizer from sklearn feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#importing gensim for vector embeddings and importing \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#importing minmax scaler from sklearn preprocessing for normalize the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#importing the base models from the sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604eaf6a",
   "metadata": {
    "id": "604eaf6a"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">1 A - Clearly write outcome of data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da9b753",
   "metadata": {
    "id": "6da9b753"
   },
   "outputs": [],
   "source": [
    "# reading blogtext csv unsinf pandas\n",
    "\n",
    "blogtxt = pd.read_csv('blogtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec611bfb",
   "metadata": {
    "id": "ec611bfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogtxt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fef1d8f",
   "metadata": {
    "id": "8fef1d8f"
   },
   "outputs": [],
   "source": [
    "#making copy of the dataset to workon\n",
    "data = blogtxt.sample(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf95c8c",
   "metadata": {
    "id": "ddf95c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1. Shape Report:\u001b[0m \n",
      "\tThe Dataset as 30000 Rows and 7 Columns\n",
      "\n",
      "\u001b[0\u0001m2. Data's in the Dataset\u001b[0m\n",
      "\n",
      "\u001b[1m2.1 First 5 Rows from the dataset:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320259</th>\n",
       "      <td>947299</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Law</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>24,August,2003</td>\n",
       "      <td>WWE Summer Slam was phenominal, here's my r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111750</th>\n",
       "      <td>1662633</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>22,June,2004</td>\n",
       "      <td>we must have been posting at the same t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180457</th>\n",
       "      <td>1093457</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>22,July,2003</td>\n",
       "      <td>I'm bored, Ben is cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458390</th>\n",
       "      <td>518116</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>29,March,2004</td>\n",
       "      <td>I'm so glad that worked!  Anyway...so s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581214</th>\n",
       "      <td>3387580</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>18,May,2004</td>\n",
       "      <td>I figured it out this time but I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age        topic    sign            date  \\\n",
       "320259   947299    male   24          Law  Gemini  24,August,2003   \n",
       "111750  1662633  female   23  Engineering  Pisces    22,June,2004   \n",
       "180457  1093457    male   16   Technology   Virgo    22,July,2003   \n",
       "458390   518116  female   27       indUnk  Pisces   29,March,2004   \n",
       "581214  3387580  female   33       indUnk   Virgo     18,May,2004   \n",
       "\n",
       "                                                     text  \n",
       "320259     WWE Summer Slam was phenominal, here's my r...  \n",
       "111750         we must have been posting at the same t...  \n",
       "180457                    I'm bored, Ben is cool           \n",
       "458390         I'm so glad that worked!  Anyway...so s...  \n",
       "581214               I figured it out this time but I ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2.2 Last 5 Rows from the dataset\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367097</th>\n",
       "      <td>1119280</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>01,August,2004</td>\n",
       "      <td>This afternoon I got a phone call from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551038</th>\n",
       "      <td>1125546</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Law</td>\n",
       "      <td>Aries</td>\n",
       "      <td>20,July,2004</td>\n",
       "      <td>Though a lot of lawyers are themselves ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244020</th>\n",
       "      <td>1506385</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>11,May,2004</td>\n",
       "      <td>Haven't seen a bollywood flick in 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438062</th>\n",
       "      <td>1853281</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>02,October,2003</td>\n",
       "      <td>I know, I know bad poetry, i promised i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139272</th>\n",
       "      <td>3552447</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>14,July,2004</td>\n",
       "      <td>umm...i forgot what i was gonna put her...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age     topic      sign             date  \\\n",
       "367097  1119280  female   27    indUnk    Gemini   01,August,2004   \n",
       "551038  1125546    male   27       Law     Aries     20,July,2004   \n",
       "244020  1506385  female   27  Internet    Gemini      11,May,2004   \n",
       "438062  1853281    male   23   Student    Cancer  02,October,2003   \n",
       "139272  3552447    male   15   Student  Aquarius     14,July,2004   \n",
       "\n",
       "                                                     text  \n",
       "367097         This afternoon I got a phone call from ...  \n",
       "551038         Though a lot of lawyers are themselves ...  \n",
       "244020             Haven't seen a bollywood flick in 2...  \n",
       "438062         I know, I know bad poetry, i promised i...  \n",
       "139272         umm...i forgot what i was gonna put her...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3. Infomation about data in the dataset:\u001b[0m\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 320259 to 139272\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      30000 non-null  int64 \n",
      " 1   gender  30000 non-null  object\n",
      " 2   age     30000 non-null  int64 \n",
      " 3   topic   30000 non-null  object\n",
      " 4   sign    30000 non-null  object\n",
      " 5   date    30000 non-null  object\n",
      " 6   text    30000 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 1.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4. Distribution of the Numberic Columns:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>30000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.401867e+06</td>\n",
       "      <td>23.99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.244008e+06</td>\n",
       "      <td>7.85341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.114000e+03</td>\n",
       "      <td>13.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.246850e+06</td>\n",
       "      <td>17.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.615621e+06</td>\n",
       "      <td>24.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.525448e+06</td>\n",
       "      <td>27.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.334761e+06</td>\n",
       "      <td>48.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id          age\n",
       "count  3.000000e+04  30000.00000\n",
       "mean   2.401867e+06     23.99980\n",
       "std    1.244008e+06      7.85341\n",
       "min    5.114000e+03     13.00000\n",
       "25%    1.246850e+06     17.00000\n",
       "50%    2.615621e+06     24.00000\n",
       "75%    3.525448e+06     27.00000\n",
       "max    4.334761e+06     48.00000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01m5. Number of Duplicate Entries in the dataset:\u001b[0m 33\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "#size of dataset\n",
    "shape = data.shape\n",
    "print(\"\\033[1m1. Shape Report:\\033[0m \\n\\tThe Dataset as {} Rows and {} Columns\\n\".format(shape[0],shape[1]))\n",
    "\n",
    "#Datas\n",
    "print(\"\\033[0\\1m2. Data's in the Dataset\\033[0m\\n\")\n",
    "#Presenting the data\n",
    "print(\"\\033[1m2.1 First 5 Rows from the dataset:\\033[0m\")\n",
    "display(data.head(5))\n",
    "print(\"\\033[1m2.2 Last 5 Rows from the dataset\\033[0m\")\n",
    "display(data.tail(5))\n",
    "\n",
    "#Info\n",
    "print(\"\\033[1m3. Infomation about data in the dataset:\\033[0m\")\n",
    "display(data.info())\n",
    "\n",
    "#describe\n",
    "print('\\033[1m4. Distribution of the Numberic Columns:\\033[0m')\n",
    "display(data.describe())\n",
    "\n",
    "#duplicate\n",
    "print('\\033[01m5. Number of Duplicate Entries in the dataset:\\033[0m {}'.format(data.duplicated().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6e460",
   "metadata": {
    "id": "c1f6e460"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3845C7;\"> Observations: </span> \n",
    "\n",
    "1. The Dataset blog text as 681284 Rows and 7 Columns reducing the data by sample 20000 rows\n",
    "2. Columns Details:\n",
    "\n",
    "    2.1 In the 7 columns, 2 int64 datatype id (unique number of blog) & age (age of the person making the blog)\n",
    "    \n",
    "    2.2 Remaining are the object data type columns gender(gender of a person), topic(the blog based on), sign(posted name), date (date posted), text(context onf the blog)\n",
    "3. for information function that there is not Null values in the dataset and columns\n",
    "4. 4686 duplicate entiries in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb9ba16",
   "metadata": {
    "id": "1fb9ba16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Law' 'Engineering' 'Technology' 'indUnk' 'LawEnforcement-Security'\n",
      " 'Fashion' 'Communications-Media' 'Student' 'Internet' 'Arts' 'Government'\n",
      " 'Publishing' 'Non-Profit' 'Education' 'RealEstate' 'Banking' 'Marketing'\n",
      " 'Tourism' 'Science' 'HumanResources' 'Religion' 'BusinessServices'\n",
      " 'Consulting' 'Chemicals' 'Agriculture' 'Automotive' 'Accounting'\n",
      " 'Advertising' 'Sports-Recreation' 'Military' 'Construction' 'Biotech'\n",
      " 'Architecture' 'InvestmentBanking' 'Telecommunications' 'Environment'\n",
      " 'Transportation' 'Manufacturing' 'Museums-Libraries' 'Maritime']\n"
     ]
    }
   ],
   "source": [
    "#finding unique entire in the traget columns\n",
    "unique = data['topic'].unique()\n",
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e17c47",
   "metadata": {
    "id": "e6e17c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189530f",
   "metadata": {
    "id": "b189530f"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3845C7;\"> Observations: </span> The Target Columns topic as 40 different topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb39e5",
   "metadata": {
    "id": "7abb39e5"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">1 B - Clean the Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33a1f061",
   "metadata": {
    "id": "33a1f061"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "gender    0\n",
       "age       0\n",
       "topic     0\n",
       "sign      0\n",
       "date      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b418ce4",
   "metadata": {
    "id": "0b418ce4"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3845C7;\"> Observations: </span> No Null Values is observered from the isna function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5691a287",
   "metadata": {
    "id": "5691a287"
   },
   "outputs": [],
   "source": [
    "#defining function to detect the lang of the text column\n",
    "#passing data\n",
    "def detect_english(data):\n",
    "    try:\n",
    "        return detect(data) == 'en' #detecting lang is en\n",
    "    except:\n",
    "        return False #else return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2cc708c",
   "metadata": {
    "id": "d2cc708c"
   },
   "outputs": [],
   "source": [
    "data['lang'] = data['text'].apply(detect_english) #adding lang column in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f958b61",
   "metadata": {
    "id": "4f958b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non English Text: 1351\n"
     ]
    }
   ],
   "source": [
    "noneng = data.loc[data['lang'] == False].index #making the list of indexes of non english\n",
    "print('Non English Text:',len(noneng)) #printing the length of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd2e1229",
   "metadata": {
    "id": "fd2e1229"
   },
   "outputs": [],
   "source": [
    "data = data.drop(noneng) #droping the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e283f23",
   "metadata": {
    "id": "4e283f23"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">2 A - Eliminate All special Characters and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06dae99e",
   "metadata": {
    "id": "06dae99e"
   },
   "outputs": [],
   "source": [
    "#function for removing special character and number \n",
    "def remove_sn(data):\n",
    "    return re.sub(r\"[^a-zA-Z]\", \" \",data)\n",
    "\n",
    "data['Clean Text'] = data['text'].apply(remove_sn) #applying the function to the text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b969f44e",
   "metadata": {
    "id": "b969f44e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>Clean Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320259</th>\n",
       "      <td>947299</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Law</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>24,August,2003</td>\n",
       "      <td>WWE Summer Slam was phenominal, here's my r...</td>\n",
       "      <td>True</td>\n",
       "      <td>WWE Summer Slam was phenominal  here s my r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111750</th>\n",
       "      <td>1662633</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>22,June,2004</td>\n",
       "      <td>we must have been posting at the same t...</td>\n",
       "      <td>True</td>\n",
       "      <td>we must have been posting at the same t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180457</th>\n",
       "      <td>1093457</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>22,July,2003</td>\n",
       "      <td>I'm bored, Ben is cool</td>\n",
       "      <td>True</td>\n",
       "      <td>I m bored  Ben is cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458390</th>\n",
       "      <td>518116</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>29,March,2004</td>\n",
       "      <td>I'm so glad that worked!  Anyway...so s...</td>\n",
       "      <td>True</td>\n",
       "      <td>I m so glad that worked   Anyway   so s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581214</th>\n",
       "      <td>3387580</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>18,May,2004</td>\n",
       "      <td>I figured it out this time but I ...</td>\n",
       "      <td>True</td>\n",
       "      <td>I figured it out this time but I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age        topic    sign            date  \\\n",
       "320259   947299    male   24          Law  Gemini  24,August,2003   \n",
       "111750  1662633  female   23  Engineering  Pisces    22,June,2004   \n",
       "180457  1093457    male   16   Technology   Virgo    22,July,2003   \n",
       "458390   518116  female   27       indUnk  Pisces   29,March,2004   \n",
       "581214  3387580  female   33       indUnk   Virgo     18,May,2004   \n",
       "\n",
       "                                                     text  lang  \\\n",
       "320259     WWE Summer Slam was phenominal, here's my r...  True   \n",
       "111750         we must have been posting at the same t...  True   \n",
       "180457                    I'm bored, Ben is cool           True   \n",
       "458390         I'm so glad that worked!  Anyway...so s...  True   \n",
       "581214               I figured it out this time but I ...  True   \n",
       "\n",
       "                                               Clean Text  \n",
       "320259     WWE Summer Slam was phenominal  here s my r...  \n",
       "111750         we must have been posting at the same t...  \n",
       "180457                    I m bored  Ben is cool           \n",
       "458390         I m so glad that worked   Anyway   so s...  \n",
       "581214               I figured it out this time but I ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90628f",
   "metadata": {
    "id": "df90628f"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">2 B - Lowercase all textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "278e672e",
   "metadata": {
    "id": "278e672e"
   },
   "outputs": [],
   "source": [
    "#function for changing the text into lower case\n",
    "def remove_lc(data):\n",
    "    return data.lower()\n",
    "\n",
    "data['Clean Text'] = data['Clean Text'].apply(remove_lc) #applying the function to the clear text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a351065",
   "metadata": {
    "id": "4a351065"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>Clean Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320259</th>\n",
       "      <td>947299</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Law</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>24,August,2003</td>\n",
       "      <td>WWE Summer Slam was phenominal, here's my r...</td>\n",
       "      <td>True</td>\n",
       "      <td>wwe summer slam was phenominal  here s my r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111750</th>\n",
       "      <td>1662633</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>22,June,2004</td>\n",
       "      <td>we must have been posting at the same t...</td>\n",
       "      <td>True</td>\n",
       "      <td>we must have been posting at the same t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180457</th>\n",
       "      <td>1093457</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>22,July,2003</td>\n",
       "      <td>I'm bored, Ben is cool</td>\n",
       "      <td>True</td>\n",
       "      <td>i m bored  ben is cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458390</th>\n",
       "      <td>518116</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>29,March,2004</td>\n",
       "      <td>I'm so glad that worked!  Anyway...so s...</td>\n",
       "      <td>True</td>\n",
       "      <td>i m so glad that worked   anyway   so s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581214</th>\n",
       "      <td>3387580</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>18,May,2004</td>\n",
       "      <td>I figured it out this time but I ...</td>\n",
       "      <td>True</td>\n",
       "      <td>i figured it out this time but i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age        topic    sign            date  \\\n",
       "320259   947299    male   24          Law  Gemini  24,August,2003   \n",
       "111750  1662633  female   23  Engineering  Pisces    22,June,2004   \n",
       "180457  1093457    male   16   Technology   Virgo    22,July,2003   \n",
       "458390   518116  female   27       indUnk  Pisces   29,March,2004   \n",
       "581214  3387580  female   33       indUnk   Virgo     18,May,2004   \n",
       "\n",
       "                                                     text  lang  \\\n",
       "320259     WWE Summer Slam was phenominal, here's my r...  True   \n",
       "111750         we must have been posting at the same t...  True   \n",
       "180457                    I'm bored, Ben is cool           True   \n",
       "458390         I'm so glad that worked!  Anyway...so s...  True   \n",
       "581214               I figured it out this time but I ...  True   \n",
       "\n",
       "                                               Clean Text  \n",
       "320259     wwe summer slam was phenominal  here s my r...  \n",
       "111750         we must have been posting at the same t...  \n",
       "180457                    i m bored  ben is cool           \n",
       "458390         i m so glad that worked   anyway   so s...  \n",
       "581214               i figured it out this time but i ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3ec8d",
   "metadata": {
    "id": "2ec3ec8d"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">2 C - Remove all Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2043399f",
   "metadata": {
    "id": "2043399f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18min 21s\n",
      "Wall time: 22min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#function for removing the stopwords\n",
    "def remove_sw(data):\n",
    "    return ' '.join([word for word in data.split() if word not in stopwords.words('english')])\n",
    "\n",
    "data['Clean Text'] = data['Clean Text'].apply(remove_sw) #applying the function to the clear Text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dfdf536",
   "metadata": {
    "id": "5dfdf536"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>Clean Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320259</th>\n",
       "      <td>947299</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Law</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>24,August,2003</td>\n",
       "      <td>WWE Summer Slam was phenominal, here's my r...</td>\n",
       "      <td>True</td>\n",
       "      <td>wwe summer slam phenominal review first match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111750</th>\n",
       "      <td>1662633</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>22,June,2004</td>\n",
       "      <td>we must have been posting at the same t...</td>\n",
       "      <td>True</td>\n",
       "      <td>must posting time cause almost lost entire blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180457</th>\n",
       "      <td>1093457</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>22,July,2003</td>\n",
       "      <td>I'm bored, Ben is cool</td>\n",
       "      <td>True</td>\n",
       "      <td>bored ben cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458390</th>\n",
       "      <td>518116</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>29,March,2004</td>\n",
       "      <td>I'm so glad that worked!  Anyway...so s...</td>\n",
       "      <td>True</td>\n",
       "      <td>glad worked anyway sis make britney spears con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581214</th>\n",
       "      <td>3387580</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>18,May,2004</td>\n",
       "      <td>I figured it out this time but I ...</td>\n",
       "      <td>True</td>\n",
       "      <td>figured time figure find place put post illite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age        topic    sign            date  \\\n",
       "320259   947299    male   24          Law  Gemini  24,August,2003   \n",
       "111750  1662633  female   23  Engineering  Pisces    22,June,2004   \n",
       "180457  1093457    male   16   Technology   Virgo    22,July,2003   \n",
       "458390   518116  female   27       indUnk  Pisces   29,March,2004   \n",
       "581214  3387580  female   33       indUnk   Virgo     18,May,2004   \n",
       "\n",
       "                                                     text  lang  \\\n",
       "320259     WWE Summer Slam was phenominal, here's my r...  True   \n",
       "111750         we must have been posting at the same t...  True   \n",
       "180457                    I'm bored, Ben is cool           True   \n",
       "458390         I'm so glad that worked!  Anyway...so s...  True   \n",
       "581214               I figured it out this time but I ...  True   \n",
       "\n",
       "                                               Clean Text  \n",
       "320259  wwe summer slam phenominal review first match ...  \n",
       "111750  must posting time cause almost lost entire blo...  \n",
       "180457                                     bored ben cool  \n",
       "458390  glad worked anyway sis make britney spears con...  \n",
       "581214  figured time figure find place put post illite...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137f6de",
   "metadata": {
    "id": "4137f6de"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">2 D - Remove all extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4833153a",
   "metadata": {
    "id": "4833153a"
   },
   "outputs": [],
   "source": [
    "data['Clean Text'] = data['Clean Text'].str.strip() #removing extra white spaces from the text using str strip function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a56bcf07",
   "metadata": {
    "id": "a56bcf07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>Clean Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320259</th>\n",
       "      <td>947299</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Law</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>24,August,2003</td>\n",
       "      <td>WWE Summer Slam was phenominal, here's my r...</td>\n",
       "      <td>True</td>\n",
       "      <td>wwe summer slam phenominal review first match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111750</th>\n",
       "      <td>1662633</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>22,June,2004</td>\n",
       "      <td>we must have been posting at the same t...</td>\n",
       "      <td>True</td>\n",
       "      <td>must posting time cause almost lost entire blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180457</th>\n",
       "      <td>1093457</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>22,July,2003</td>\n",
       "      <td>I'm bored, Ben is cool</td>\n",
       "      <td>True</td>\n",
       "      <td>bored ben cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458390</th>\n",
       "      <td>518116</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>29,March,2004</td>\n",
       "      <td>I'm so glad that worked!  Anyway...so s...</td>\n",
       "      <td>True</td>\n",
       "      <td>glad worked anyway sis make britney spears con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581214</th>\n",
       "      <td>3387580</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>18,May,2004</td>\n",
       "      <td>I figured it out this time but I ...</td>\n",
       "      <td>True</td>\n",
       "      <td>figured time figure find place put post illite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age        topic    sign            date  \\\n",
       "320259   947299    male   24          Law  Gemini  24,August,2003   \n",
       "111750  1662633  female   23  Engineering  Pisces    22,June,2004   \n",
       "180457  1093457    male   16   Technology   Virgo    22,July,2003   \n",
       "458390   518116  female   27       indUnk  Pisces   29,March,2004   \n",
       "581214  3387580  female   33       indUnk   Virgo     18,May,2004   \n",
       "\n",
       "                                                     text  lang  \\\n",
       "320259     WWE Summer Slam was phenominal, here's my r...  True   \n",
       "111750         we must have been posting at the same t...  True   \n",
       "180457                    I'm bored, Ben is cool           True   \n",
       "458390         I'm so glad that worked!  Anyway...so s...  True   \n",
       "581214               I figured it out this time but I ...  True   \n",
       "\n",
       "                                               Clean Text  \n",
       "320259  wwe summer slam phenominal review first match ...  \n",
       "111750  must posting time cause almost lost entire blo...  \n",
       "180457                                     bored ben cool  \n",
       "458390  glad worked anyway sis make britney spears con...  \n",
       "581214  figured time figure find place put post illite...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067bf18",
   "metadata": {
    "id": "6067bf18"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">3 A - Create dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e8f6bd6",
   "metadata": {
    "id": "8e8f6bd6"
   },
   "outputs": [],
   "source": [
    "#making the text as depenent variable & the Topic of the text into independent variable \n",
    "X = data['Clean Text']\n",
    "y = data['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94302147",
   "metadata": {
    "id": "94302147"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder() #creating instance of the label encoder function\n",
    "le.fit(y) #fit the target data into the labelencoder instance\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_))) #making label name mapping dictionary by zipping the class and transformer class values from label encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "162d6656",
   "metadata": {
    "id": "162d6656"
   },
   "outputs": [],
   "source": [
    "y = y.map(le_name_mapping) #mapping y value as y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e36ca",
   "metadata": {
    "id": "e16e36ca"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">3 B - Split data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b70c0099",
   "metadata": {
    "id": "b70c0099"
   },
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.3,random_state=12) #creating the training and testing dataset using train_test_split function from X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ce14310",
   "metadata": {
    "id": "9ce14310"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20054,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8595,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20054,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8595,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#printing the shapes of all the training and testing dataset\n",
    "display(Xtrain.shape)\n",
    "display(Xtest.shape)\n",
    "display(ytrain.shape)\n",
    "display(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1983b",
   "metadata": {
    "id": "05b1983b"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">3 C - Vectorize data using any one vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bab02ce1",
   "metadata": {
    "id": "bab02ce1"
   },
   "outputs": [],
   "source": [
    "#creating instances for the counter vectorizer function with parameters Max feature and ngram range\n",
    "cv = CountVectorizer(max_features=5000,ngram_range=(1,4))\n",
    "X_train = cv.fit_transform(Xtrain).toarray() #transforming the Xtrain using count vectorizer function and making it as array\n",
    "X_test = cv.fit_transform(Xtest).toarray() #transforming the Xtest using count vectorizer function and making it as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fcdcfe3",
   "metadata": {
    "id": "4fcdcfe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'well': 4805,\n",
       " 'back': 293,\n",
       " 'weeks': 4800,\n",
       " 'cell': 630,\n",
       " 'phone': 3251,\n",
       " 'anyone': 171,\n",
       " 'much': 2869,\n",
       " 'give': 1756,\n",
       " 'ring': 3681,\n",
       " 'ta': 4289,\n",
       " 'cell phone': 631,\n",
       " 'yeah': 4966,\n",
       " 'last': 2398,\n",
       " 'couple': 917,\n",
       " 'days': 1019,\n",
       " 'went': 4820,\n",
       " 'see': 3828,\n",
       " 'matt': 2710,\n",
       " 'sharp': 3897,\n",
       " 'play': 3298,\n",
       " 'sunday': 4248,\n",
       " 'fun': 1679,\n",
       " 'except': 1394,\n",
       " 'fact': 1439,\n",
       " 'girlfriend': 1754,\n",
       " 'know': 2344,\n",
       " 'honestly': 2043,\n",
       " 'happy': 1942,\n",
       " 'whole': 4841,\n",
       " 'hell': 1986,\n",
       " 'lot': 2606,\n",
       " 'better': 391,\n",
       " 'seeing': 3834,\n",
       " 'must': 2884,\n",
       " 'say': 3775,\n",
       " 'nicely': 2981,\n",
       " 'got': 1832,\n",
       " 'girl': 1753,\n",
       " 'keeping': 2301,\n",
       " 'hair': 1920,\n",
       " 'cut': 973,\n",
       " 'wearing': 4784,\n",
       " 'nice': 2980,\n",
       " 'clothing': 755,\n",
       " 'shape': 3893,\n",
       " 'every': 1369,\n",
       " 'morning': 2847,\n",
       " 'complete': 820,\n",
       " 'long': 2578,\n",
       " 'clock': 746,\n",
       " 'ripped': 3685,\n",
       " 'dirty': 1129,\n",
       " 'clothes': 754,\n",
       " 'guy': 1905,\n",
       " 'saw': 3774,\n",
       " 'year': 4967,\n",
       " 'ago': 76,\n",
       " 'jealous': 2239,\n",
       " 'make': 2655,\n",
       " 'eyes': 1430,\n",
       " 'smile': 4011,\n",
       " 'really': 3550,\n",
       " 'wish': 4873,\n",
       " 'could': 898,\n",
       " 'damn': 985,\n",
       " 'thing': 4387,\n",
       " 'like': 2500,\n",
       " 'knows': 2365,\n",
       " 'anything': 174,\n",
       " 'flat': 1570,\n",
       " 'away': 283,\n",
       " 'probably': 3417,\n",
       " 'one': 3070,\n",
       " 'anyways': 179,\n",
       " 'would': 4923,\n",
       " 'still': 4166,\n",
       " 'kill': 2318,\n",
       " 'friend': 1656,\n",
       " 'hey': 1998,\n",
       " 'guess': 1893,\n",
       " 'world': 4911,\n",
       " 'store': 4180,\n",
       " 'depressed': 1068,\n",
       " 'lately': 2413,\n",
       " 'think': 4396,\n",
       " 'directly': 1126,\n",
       " 'feel': 1493,\n",
       " 'alone': 105,\n",
       " 'somebody': 4035,\n",
       " 'lighter': 2497,\n",
       " 'side': 3944,\n",
       " 'met': 2761,\n",
       " 'actually': 37,\n",
       " 'came': 573,\n",
       " 'sat': 3765,\n",
       " 'next': 2972,\n",
       " 'show': 3932,\n",
       " 'jen': 2242,\n",
       " 'wanted': 4749,\n",
       " 'go': 1767,\n",
       " 'home': 2037,\n",
       " 'doors': 1175,\n",
       " 'let': 2467,\n",
       " 'us': 4654,\n",
       " 'someone': 4038,\n",
       " 'come': 782,\n",
       " 'talk': 4304,\n",
       " 'get': 1716,\n",
       " 'keys': 2310,\n",
       " 'way': 4774,\n",
       " 'ny': 3026,\n",
       " 'sooo': 4055,\n",
       " 'awesome': 284,\n",
       " 'car': 596,\n",
       " 'parents': 3167,\n",
       " 'names': 2895,\n",
       " 'another': 158,\n",
       " 'story': 4185,\n",
       " 'best': 385,\n",
       " 'amazing': 120,\n",
       " 'last couple': 2399,\n",
       " 'couple days': 918,\n",
       " 'went see': 4824,\n",
       " 'must say': 2885,\n",
       " 'wish could': 4874,\n",
       " 'could make': 902,\n",
       " 'would still': 4938,\n",
       " 'wanted go': 4751,\n",
       " 'go home': 1773,\n",
       " 'let us': 2474,\n",
       " 'though': 4422,\n",
       " 'made': 2642,\n",
       " 'night': 2984,\n",
       " 'around': 217,\n",
       " 'scheduled': 3791,\n",
       " 'arrive': 221,\n",
       " 'pm': 3316,\n",
       " 'mind': 2792,\n",
       " 'computer': 825,\n",
       " 'book': 466,\n",
       " 'interesting': 2190,\n",
       " 'people': 3214,\n",
       " 'look': 2584,\n",
       " 'end': 1307,\n",
       " 'trip': 4557,\n",
       " 'even': 1356,\n",
       " 'airport': 88,\n",
       " 'friends': 1659,\n",
       " 'funny': 1684,\n",
       " 'sometimes': 4046,\n",
       " 'bad': 303,\n",
       " 'situations': 3982,\n",
       " 'good': 1803,\n",
       " 'time': 4455,\n",
       " 'getting': 1745,\n",
       " 'cool': 882,\n",
       " 'entire': 1330,\n",
       " 'steph': 4156,\n",
       " 'little': 2553,\n",
       " 'boy': 490,\n",
       " 'day': 1012,\n",
       " 'used': 4657,\n",
       " 'live': 2556,\n",
       " 'pa': 3142,\n",
       " 'visit': 4702,\n",
       " 'turning': 4580,\n",
       " 'crazy': 938,\n",
       " 'big': 396,\n",
       " 'everything': 1380,\n",
       " 'says': 3780,\n",
       " 'whenever': 4832,\n",
       " 'together': 4490,\n",
       " 'lost': 2605,\n",
       " 'already': 109,\n",
       " 'miss': 2808,\n",
       " 'sleep': 3997,\n",
       " 'since': 3962,\n",
       " 'slept': 4000,\n",
       " 'five': 1563,\n",
       " 'hours': 2064,\n",
       " 'straight': 4186,\n",
       " 'might': 2781,\n",
       " 'bed': 354,\n",
       " 'bit': 413,\n",
       " 'leaving': 2453,\n",
       " 'work': 4902,\n",
       " 'gone': 1799,\n",
       " 'full': 1676,\n",
       " 'whatever': 4828,\n",
       " 'want': 4739,\n",
       " 'reading': 3534,\n",
       " 'sleeping': 3998,\n",
       " 'running': 3727,\n",
       " 'almost': 104,\n",
       " 'done': 1167,\n",
       " 'writing': 4950,\n",
       " 'beautiful': 347,\n",
       " 'finish': 1546,\n",
       " 'life': 2489,\n",
       " 'last night': 2402,\n",
       " 'good time': 1821,\n",
       " 'know people': 2354,\n",
       " 'got see': 1839,\n",
       " 'way back': 4775,\n",
       " 'last time': 2406,\n",
       " 'even though': 1361,\n",
       " 'get back': 1719,\n",
       " 'think might': 4405,\n",
       " 'might go': 2782,\n",
       " 'go back': 1770,\n",
       " 'back bed': 294,\n",
       " 'little bit': 2554,\n",
       " 'whatever want': 4829,\n",
       " 'really like': 3561,\n",
       " 'tea': 4324,\n",
       " 'mail': 2648,\n",
       " 'today': 4482,\n",
       " 'wonderful': 4887,\n",
       " 'summer': 4246,\n",
       " 'lots': 2610,\n",
       " 'supposed': 4259,\n",
       " 'yummy': 4995,\n",
       " 'green': 1874,\n",
       " 'types': 4596,\n",
       " 'throat': 4436,\n",
       " 'perfect': 3230,\n",
       " 'find': 1539,\n",
       " 'strong': 4202,\n",
       " 'earlier': 1240,\n",
       " 'weak': 4780,\n",
       " 'taste': 4315,\n",
       " 'mostly': 2849,\n",
       " 'light': 2496,\n",
       " 'making': 2665,\n",
       " 'new': 2963,\n",
       " 'soon': 4053,\n",
       " 'reach': 3525,\n",
       " 'grab': 1846,\n",
       " 'drink': 1201,\n",
       " 'months': 2839,\n",
       " 'urllink': 4645,\n",
       " 'ok': 3062,\n",
       " 'take': 4294,\n",
       " 'shower': 3934,\n",
       " 'eat': 1253,\n",
       " 'bowl': 486,\n",
       " 'mini': 2796,\n",
       " 'heading': 1964,\n",
       " 'try': 4566,\n",
       " 'first': 1553,\n",
       " 'always': 116,\n",
       " 'worst': 4920,\n",
       " 'weed': 4795,\n",
       " 'ones': 3096,\n",
       " 'hopefully': 2053,\n",
       " 'season': 3812,\n",
       " 'bring': 514,\n",
       " 'wins': 4867,\n",
       " 'although': 115,\n",
       " 'said': 3747,\n",
       " 'jennifer': 2243,\n",
       " 'gets': 1743,\n",
       " 'mood': 2842,\n",
       " 'forward': 1623,\n",
       " 'spending': 4093,\n",
       " 'girls': 1755,\n",
       " 'aka': 89,\n",
       " 'coach': 764,\n",
       " 'split': 4103,\n",
       " 'groups': 1885,\n",
       " 'gave': 1701,\n",
       " 'speech': 4087,\n",
       " 'broke': 519,\n",
       " 'saying': 3779,\n",
       " 'division': 1152,\n",
       " 'team': 4329,\n",
       " 'personally': 3241,\n",
       " 'totally': 4517,\n",
       " 'obvious': 3031,\n",
       " 'correct': 891,\n",
       " 'far': 1470,\n",
       " 'goes': 1784,\n",
       " 'calling': 570,\n",
       " 'name': 2893,\n",
       " 'bye': 560,\n",
       " 'first day': 1555,\n",
       " 'like said': 2516,\n",
       " 'look forward': 2586,\n",
       " 'lot time': 2609,\n",
       " 'haha': 1914,\n",
       " 'tell': 4346,\n",
       " 'jason': 2236,\n",
       " 'message': 2757,\n",
       " 'sounds': 4068,\n",
       " 'wrong': 4952,\n",
       " 'moment': 2827,\n",
       " 'bathroom': 337,\n",
       " 'food': 1597,\n",
       " 'terrible': 4357,\n",
       " 'combination': 780,\n",
       " 'called': 568,\n",
       " 'right': 3678,\n",
       " 'eating': 1255,\n",
       " 'anyway': 178,\n",
       " 'hate': 1954,\n",
       " 'waking': 4722,\n",
       " 'sense': 3857,\n",
       " 'absolute': 7,\n",
       " 'call': 567,\n",
       " 'wake': 4721,\n",
       " 'hardly': 1949,\n",
       " 'put': 3476,\n",
       " 'words': 4900,\n",
       " 'sentence': 3861,\n",
       " 'okay': 3063,\n",
       " 'told': 4493,\n",
       " 'odd': 3039,\n",
       " 'dreams': 1196,\n",
       " 'yesterday': 4982,\n",
       " 'missed': 2812,\n",
       " 'heh': 1981,\n",
       " 'btw': 529,\n",
       " 'thrown': 4440,\n",
       " 'women': 4884,\n",
       " 'three': 4431,\n",
       " 'wishes': 4877,\n",
       " 'eh': 1277,\n",
       " 'questioning': 3485,\n",
       " 'billy': 405,\n",
       " 'hard': 1944,\n",
       " 'answer': 162,\n",
       " 'peace': 3208,\n",
       " 'things': 4389,\n",
       " 'silly': 3955,\n",
       " 'maybe': 2717,\n",
       " 'phrase': 3257,\n",
       " 'violence': 4696,\n",
       " 'degree': 1051,\n",
       " 'second': 3818,\n",
       " 'kick': 2311,\n",
       " 'court': 924,\n",
       " 'turn': 4577,\n",
       " 'music': 2881,\n",
       " 'open': 3101,\n",
       " 'third': 4416,\n",
       " 'hmmm': 2025,\n",
       " 'wait': 4716,\n",
       " 'shoes': 3914,\n",
       " 'smell': 4009,\n",
       " 'late': 2411,\n",
       " 'couple hours': 919,\n",
       " 'would say': 4937,\n",
       " 'would make': 4931,\n",
       " 'make things': 2659,\n",
       " 'know want': 2359,\n",
       " 'life would': 2490,\n",
       " 'late night': 2412,\n",
       " 'november': 3014,\n",
       " 'december': 1032,\n",
       " 'march': 2684,\n",
       " 'daily': 982,\n",
       " 'single': 3969,\n",
       " 'love': 2614,\n",
       " 'something': 4040,\n",
       " 'comes': 786,\n",
       " 'thanks': 4375,\n",
       " 'special': 4084,\n",
       " 'grateful': 1864,\n",
       " 'tasty': 4317,\n",
       " 'meal': 2720,\n",
       " 'conversation': 873,\n",
       " 'teh': 4343,\n",
       " 'ill': 2113,\n",
       " 'adam': 39,\n",
       " 'dinner': 1121,\n",
       " 'practice': 3367,\n",
       " 'lol': 2573,\n",
       " 'think something': 4411,\n",
       " 'found': 1625,\n",
       " 'www': 4955,\n",
       " 'com': 778,\n",
       " 'ask': 235,\n",
       " 'sort': 4059,\n",
       " 'partner': 3178,\n",
       " 'internet': 2193,\n",
       " 'crime': 949,\n",
       " 'also': 112,\n",
       " 'blog': 434,\n",
       " 'man': 2669,\n",
       " 'link': 2537,\n",
       " 'top': 4511,\n",
       " 'hand': 1928,\n",
       " 'social': 4024,\n",
       " 'along': 106,\n",
       " 'ten': 4352,\n",
       " 'lists': 2549,\n",
       " 'movie': 2860,\n",
       " 'tv': 4582,\n",
       " 'section': 3825,\n",
       " 'decide': 1034,\n",
       " 'purchase': 3467,\n",
       " 'recommend': 3580,\n",
       " 'buy': 558,\n",
       " 'lose': 2601,\n",
       " 'service': 3872,\n",
       " 'price': 3402,\n",
       " 'movies': 2861,\n",
       " 'current': 968,\n",
       " 'favorite': 1482,\n",
       " 'number': 3019,\n",
       " 'two': 4587,\n",
       " 'seem': 3837,\n",
       " 'vote': 4708,\n",
       " 'created': 941,\n",
       " 'list': 2544,\n",
       " 'contains': 862,\n",
       " 'dvd': 1232,\n",
       " 'passion': 3186,\n",
       " 'christ': 709,\n",
       " 'favorites': 1483,\n",
       " 'played': 3299,\n",
       " 'expect': 1406,\n",
       " 'change': 654,\n",
       " 'often': 3053,\n",
       " 'shows': 3937,\n",
       " 'exception': 1395,\n",
       " 'available': 275,\n",
       " 'mad': 2641,\n",
       " 'example': 1390,\n",
       " 'half': 1923,\n",
       " 'decade': 1031,\n",
       " 'web': 4786,\n",
       " 'surfing': 4266,\n",
       " 'added': 41,\n",
       " 'bottom': 482,\n",
       " 'page': 3149,\n",
       " 'links': 2539,\n",
       " 'websites': 4790,\n",
       " 'check': 674,\n",
       " 'matter': 2711,\n",
       " 'ever': 1366,\n",
       " 'argument': 211,\n",
       " 'wrote': 4953,\n",
       " 'song': 4050,\n",
       " 'paul': 3204,\n",
       " 'john': 2260,\n",
       " 'stood': 4174,\n",
       " 'hot': 2061,\n",
       " 'scene': 3787,\n",
       " 'surf': 4264,\n",
       " 'listed': 2545,\n",
       " 'career': 601,\n",
       " 'guide': 1896,\n",
       " 'bio': 407,\n",
       " 'hundred': 2084,\n",
       " 'talking': 4307,\n",
       " 'size': 3985,\n",
       " 'color': 774,\n",
       " 'sure': 4262,\n",
       " 'game': 1689,\n",
       " 'video': 4688,\n",
       " 'museum': 2880,\n",
       " 'alternative': 114,\n",
       " 'puts': 3477,\n",
       " 'shame': 3892,\n",
       " 'pictures': 3268,\n",
       " 'classic': 732,\n",
       " 'games': 1690,\n",
       " 'photo': 3255,\n",
       " 'site': 3977,\n",
       " 'pay': 3205,\n",
       " 'official': 3050,\n",
       " 'gives': 1759,\n",
       " 'free': 1643,\n",
       " 'register': 3591,\n",
       " 'organized': 3124,\n",
       " 'letters': 2477,\n",
       " 'female': 1511,\n",
       " 'enjoy': 1320,\n",
       " 'one two': 3093,\n",
       " 'let know': 2470,\n",
       " 'weeks ago': 4801,\n",
       " 'make sure': 2658,\n",
       " 'question': 3484,\n",
       " 'bum': 541,\n",
       " 'rush': 3730,\n",
       " 'july': 2282,\n",
       " 'josh': 2272,\n",
       " 'katie': 2297,\n",
       " 'lived': 2557,\n",
       " 'chris': 708,\n",
       " 'house': 2065,\n",
       " 'mate': 2704,\n",
       " 'maths': 2708,\n",
       " 'nine': 2988,\n",
       " 'hope': 2050,\n",
       " 'clever': 740,\n",
       " 'help': 1989,\n",
       " 'last days': 2401,\n",
       " 'playing': 3302,\n",
       " 'posted': 3351,\n",
       " 'week': 4796,\n",
       " 'school': 3793,\n",
       " 'sleepy': 3999,\n",
       " 'saturday': 3767,\n",
       " 'hour': 2063,\n",
       " 'kinda': 2324,\n",
       " 'oh': 3054,\n",
       " 'homework': 2040,\n",
       " 'golden': 1797,\n",
       " 'caught': 621,\n",
       " 'possibly': 3349,\n",
       " 'twins': 4586,\n",
       " 'looked': 2588,\n",
       " 'joe': 2257,\n",
       " 'nathan': 2900,\n",
       " 'surprise': 4268,\n",
       " 'jones': 2270,\n",
       " 'failure': 1446,\n",
       " 'sports': 4107,\n",
       " 'presence': 3380,\n",
       " 'ass': 241,\n",
       " 'horrible': 2056,\n",
       " 'possible': 3348,\n",
       " 'explain': 1415,\n",
       " 'graphics': 1861,\n",
       " 'thought': 4424,\n",
       " 'designed': 1079,\n",
       " 'austin': 271,\n",
       " 'experience': 1411,\n",
       " 'serving': 3874,\n",
       " 'balls': 313,\n",
       " 'left': 2457,\n",
       " 'bitch': 414,\n",
       " 'picked': 3264,\n",
       " 'collection': 772,\n",
       " 'watched': 4769,\n",
       " 'yet': 4983,\n",
       " 'identity': 2104,\n",
       " 'sum': 4245,\n",
       " 'fears': 1486,\n",
       " 'radio': 3502,\n",
       " 'lock': 2568,\n",
       " 'stock': 4169,\n",
       " 'smoking': 4018,\n",
       " 'um': 4605,\n",
       " 'walk': 4725,\n",
       " 'remember': 3613,\n",
       " 'tonight': 4504,\n",
       " 'kids': 2317,\n",
       " 'since last': 3963,\n",
       " 'last week': 2408,\n",
       " 'much time': 2875,\n",
       " 'saturday night': 3769,\n",
       " 'oh well': 3058,\n",
       " 'well get': 4807,\n",
       " 'get see': 1737,\n",
       " 'much like': 2872,\n",
       " 'good thing': 1819,\n",
       " 'brings': 516,\n",
       " 'boys': 492,\n",
       " 'yard': 4961,\n",
       " 'teach': 4325,\n",
       " 'charge': 664,\n",
       " 'fortune': 1621,\n",
       " 'cookie': 879,\n",
       " 'produced': 3424,\n",
       " 'evening': 1362,\n",
       " 'self': 3846,\n",
       " 'confidence': 838,\n",
       " 'makes': 2660,\n",
       " 'great': 1868,\n",
       " 'impression': 2133,\n",
       " 'others': 3130,\n",
       " 'news': 2969,\n",
       " 'indeed': 2149,\n",
       " 'considering': 856,\n",
       " 'started': 4134,\n",
       " 'job': 2255,\n",
       " 'good news': 1813,\n",
       " 'new job': 2964,\n",
       " 'fast': 1475,\n",
       " 'changes': 656,\n",
       " 'pretty': 3392,\n",
       " 'feeling': 1499,\n",
       " 'stage': 4120,\n",
       " 'relationship': 3600,\n",
       " 'non': 2994,\n",
       " 'nervous': 2951,\n",
       " 'period': 3235,\n",
       " 'important': 2129,\n",
       " 'happen': 1936,\n",
       " 'hit': 2020,\n",
       " 'train': 4538,\n",
       " 'dying': 1237,\n",
       " 'disease': 1142,\n",
       " 'situation': 3981,\n",
       " 'grew': 1876,\n",
       " 'apart': 184,\n",
       " 'became': 349,\n",
       " 'different': 1114,\n",
       " 'decided': 1035,\n",
       " 'anymore': 170,\n",
       " 'lines': 2536,\n",
       " 'changed': 655,\n",
       " 'moved': 2857,\n",
       " 'instead': 2177,\n",
       " 'telling': 4348,\n",
       " 'till': 4453,\n",
       " 'start': 4133,\n",
       " 'sending': 3855,\n",
       " 'refused': 3587,\n",
       " 'acts': 35,\n",
       " 'care': 599,\n",
       " 'kind': 2323,\n",
       " 'everyone': 1377,\n",
       " 'become': 351,\n",
       " 'sudden': 4233,\n",
       " 'reason': 3570,\n",
       " 'act': 26,\n",
       " 'shot': 3923,\n",
       " 'money': 2832,\n",
       " 'ended': 1309,\n",
       " 'roommate': 3709,\n",
       " 'cousin': 925,\n",
       " 'drugs': 1216,\n",
       " 'real': 3539,\n",
       " 'living': 2560,\n",
       " 'stole': 4170,\n",
       " 'multiple': 2877,\n",
       " 'items': 2220,\n",
       " 'including': 2143,\n",
       " 'paid': 3151,\n",
       " 'awhile': 286,\n",
       " 'never': 2955,\n",
       " 'thinking': 4414,\n",
       " 'latest': 2415,\n",
       " 'instantly': 2176,\n",
       " 'biggest': 400,\n",
       " 'fan': 1464,\n",
       " 'deal': 1024,\n",
       " 'going': 1786,\n",
       " 'move': 2856,\n",
       " 'graduated': 1852,\n",
       " 'college': 773,\n",
       " 'dumb': 1226,\n",
       " 'several': 3882,\n",
       " 'reasons': 3572,\n",
       " 'known': 2364,\n",
       " 'responsibility': 3648,\n",
       " 'mom': 2826,\n",
       " 'throughout': 4437,\n",
       " 'high': 2005,\n",
       " 'town': 4527,\n",
       " 'idea': 2100,\n",
       " 'selfish': 3847,\n",
       " 'interest': 2188,\n",
       " 'hanging': 1934,\n",
       " 'felt': 1509,\n",
       " 'friendship': 1660,\n",
       " 'pissed': 3280,\n",
       " 'decision': 1037,\n",
       " 'treat': 4547,\n",
       " 'deep': 1041,\n",
       " 'knew': 2339,\n",
       " 'october': 3038,\n",
       " 'early': 1241,\n",
       " 'thanksgiving': 4376,\n",
       " 'besides': 384,\n",
       " 'email': 1291,\n",
       " 'nothing': 3006,\n",
       " 'sent': 3860,\n",
       " 'gift': 1750,\n",
       " 'christmas': 714,\n",
       " 'letter': 2476,\n",
       " 'replied': 3626,\n",
       " 'contact': 861,\n",
       " 'month': 2838,\n",
       " 'similar': 3957,\n",
       " 'april': 203,\n",
       " 'may': 2716,\n",
       " 'fight': 1520,\n",
       " 'blue': 452,\n",
       " 'calls': 571,\n",
       " 'thinks': 4415,\n",
       " 'moving': 2862,\n",
       " 'crash': 937,\n",
       " 'bedroom': 355,\n",
       " 'apartment': 185,\n",
       " 'somewhere': 4048,\n",
       " 'bought': 483,\n",
       " 'tickets': 4447,\n",
       " 'coming': 794,\n",
       " 'else': 1288,\n",
       " 'decisions': 1038,\n",
       " 'ending': 1310,\n",
       " 'stressed': 4196,\n",
       " 'spent': 4095,\n",
       " 'old': 3065,\n",
       " 'times': 4470,\n",
       " 'remembered': 3614,\n",
       " 'less': 2464,\n",
       " 'normal': 2999,\n",
       " 'scared': 3785,\n",
       " 'front': 1666,\n",
       " 'upset': 4641,\n",
       " 'tried': 4555,\n",
       " 'spend': 4091,\n",
       " 'failed': 1444,\n",
       " 'strange': 4187,\n",
       " 'family': 1461,\n",
       " 'raised': 3507,\n",
       " 'religious': 3610,\n",
       " 'grace': 1848,\n",
       " 'constant': 857,\n",
       " 'loved': 2618,\n",
       " 'monkey': 2835,\n",
       " 'sex': 3884,\n",
       " 'ready': 3536,\n",
       " 'leave': 2451,\n",
       " 'meet': 2735,\n",
       " 'promised': 3441,\n",
       " 'machine': 2639,\n",
       " 'heard': 1970,\n",
       " 'voice': 4705,\n",
       " 'june': 2288,\n",
       " 'send': 3854,\n",
       " 'lake': 2383,\n",
       " 'th': 4368,\n",
       " 'mentioned': 2751,\n",
       " 'trying': 4569,\n",
       " 'starting': 4136,\n",
       " 'planning': 3292,\n",
       " 'vegas': 4677,\n",
       " 'january': 2233,\n",
       " 'reply': 3627,\n",
       " 'read': 3529,\n",
       " 'learn': 2445,\n",
       " 'suppose': 4258,\n",
       " 'proposed': 3446,\n",
       " 'sad': 3736,\n",
       " 'reaction': 3528,\n",
       " 'waited': 4719,\n",
       " 'hear': 1969,\n",
       " 'hardest': 1948,\n",
       " 'written': 4951,\n",
       " 'dropped': 1212,\n",
       " 'shock': 3911,\n",
       " 'value': 4671,\n",
       " 'actual': 36,\n",
       " 'feelings': 1501,\n",
       " 'asking': 237,\n",
       " 'without': 4880,\n",
       " 'hearing': 1971,\n",
       " 'post': 3350,\n",
       " 'seemed': 3839,\n",
       " 'point': 3321,\n",
       " 'save': 3771,\n",
       " 'finally': 1534,\n",
       " 'changing': 657,\n",
       " 'garbage': 1694,\n",
       " 'main': 2650,\n",
       " 'sounded': 4066,\n",
       " 'able': 2,\n",
       " 'total': 4516,\n",
       " 'truly': 4563,\n",
       " 'allow': 100,\n",
       " 'meant': 2726,\n",
       " 'rid': 3673,\n",
       " 'mean': 2722,\n",
       " 'true': 4562,\n",
       " 'hang': 1933,\n",
       " 'hurts': 2093,\n",
       " 'stuff': 4216,\n",
       " 'sometime': 4045,\n",
       " 'sound': 4064,\n",
       " 'enough': 1324,\n",
       " 'stupid': 4219,\n",
       " 'close': 747,\n",
       " 'bigger': 399,\n",
       " 'suck': 4229,\n",
       " 'pride': 3404,\n",
       " 'years': 4969,\n",
       " 'throw': 4438,\n",
       " 'willing': 4857,\n",
       " 'none': 2995,\n",
       " 'keep': 2299,\n",
       " 'co': 762,\n",
       " 'time since': 4466,\n",
       " 'pretty cool': 3393,\n",
       " 'months ago': 2840,\n",
       " 'wait till': 4718,\n",
       " 'really want': 3567,\n",
       " 'something like': 4042,\n",
       " 'really good': 3556,\n",
       " 'good friends': 1807,\n",
       " 'another one': 160,\n",
       " 'act like': 27,\n",
       " 'like big': 2502,\n",
       " 'whole thing': 4845,\n",
       " 'got new': 1837,\n",
       " 'makes happy': 2662,\n",
       " 'one day': 3074,\n",
       " 'whole life': 4843,\n",
       " 'high school': 2006,\n",
       " 'full time': 1677,\n",
       " 'first time': 1559,\n",
       " 'seem like': 3838,\n",
       " 'best friend': 386,\n",
       " 'want go': 4741,\n",
       " 'thought would': 4426,\n",
       " 'would never': 4932,\n",
       " 'never knew': 2958,\n",
       " 'even get': 1358,\n",
       " 'anything else': 175,\n",
       " 'take time': 4297,\n",
       " 'maybe even': 2718,\n",
       " 'much fun': 2871,\n",
       " 'really think': 3565,\n",
       " 'think anything': 4397,\n",
       " 'felt like': 1510,\n",
       " 'got back': 1833,\n",
       " 'trying get': 4572,\n",
       " 'couple weeks': 920,\n",
       " 'make feel': 2656,\n",
       " 'next week': 2977,\n",
       " 'even know': 1359,\n",
       " 'know things': 2357,\n",
       " 'pretty much': 3396,\n",
       " 'like lot': 2510,\n",
       " 'really hard': 3558,\n",
       " 'like know': 2507,\n",
       " 'want say': 4745,\n",
       " 'know know': 2347,\n",
       " 'love love': 2616,\n",
       " 'going make': 1791,\n",
       " 'everyone else': 1378,\n",
       " 'version': 4681,\n",
       " 'resist': 3642,\n",
       " 'dr': 1185,\n",
       " 'turned': 4579,\n",
       " 'died': 1108,\n",
       " 'advertising': 59,\n",
       " 'drew': 1200,\n",
       " 'children': 694,\n",
       " 'books': 467,\n",
       " 'ads': 51,\n",
       " 'skin': 3990,\n",
       " 'products': 3428,\n",
       " 'toward': 4524,\n",
       " 'hop': 2049,\n",
       " 'pop': 3335,\n",
       " 'association': 246,\n",
       " 'plastic': 3296,\n",
       " 'afraid': 66,\n",
       " 'drop': 1211,\n",
       " 'arms': 215,\n",
       " 'shake': 3889,\n",
       " 'surgery': 4267,\n",
       " 'lets': 2475,\n",
       " 'small': 4006,\n",
       " 'procedure': 3420,\n",
       " 'heaven': 1976,\n",
       " 'touch': 4518,\n",
       " 'toes': 4487,\n",
       " 'fit': 1561,\n",
       " 'inside': 2169,\n",
       " 'camping': 580,\n",
       " 'death': 1028,\n",
       " 'rate': 3517,\n",
       " 'percent': 3228,\n",
       " 'series': 3866,\n",
       " 'fish': 1560,\n",
       " 'red': 3583,\n",
       " 'club': 758,\n",
       " 'cat': 615,\n",
       " 'hat': 1953,\n",
       " 'pocket': 3318,\n",
       " 'michael': 2772,\n",
       " 'jackson': 2225,\n",
       " 'michael jackson': 2773,\n",
       " 'group': 1884,\n",
       " 'discussing': 1140,\n",
       " 'moments': 2828,\n",
       " 'figured': 1524,\n",
       " 'share': 3894,\n",
       " 'net': 2952,\n",
       " 'stories': 4183,\n",
       " 'sister': 3974,\n",
       " 'four': 1628,\n",
       " 'born': 474,\n",
       " 'six': 3983,\n",
       " 'began': 359,\n",
       " 'speak': 4081,\n",
       " 'ease': 1246,\n",
       " 'mouth': 2855,\n",
       " 'word': 4899,\n",
       " 'however': 2069,\n",
       " 'appeared': 192,\n",
       " 'manage': 2670,\n",
       " 'correctly': 892,\n",
       " 'passed': 3183,\n",
       " 'lips': 2542,\n",
       " 'stuck': 4209,\n",
       " 'later': 2414,\n",
       " 'hi': 2001,\n",
       " 'asked': 236,\n",
       " 'mother': 2850,\n",
       " 'useless': 4659,\n",
       " 'suddenly': 4234,\n",
       " 'believe': 368,\n",
       " 'apparently': 188,\n",
       " 'cartoon': 609,\n",
       " 'kid': 2315,\n",
       " 'space': 4076,\n",
       " 'showed': 3933,\n",
       " 'episode': 1337,\n",
       " 'watching': 4770,\n",
       " 'baby': 291,\n",
       " 'place': 3284,\n",
       " 'ears': 1244,\n",
       " 'science': 3795,\n",
       " 'fiction': 1515,\n",
       " 'race': 3499,\n",
       " 'interested': 2189,\n",
       " 'http': 2073,\n",
       " 'childhood': 693,\n",
       " 'happened': 1937,\n",
       " 'grade': 1849,\n",
       " 'rather': 3519,\n",
       " 'innocent': 2166,\n",
       " 'birds': 410,\n",
       " 'supposedly': 4260,\n",
       " 'men': 2747,\n",
       " 'course': 923,\n",
       " 'assumed': 248,\n",
       " 'families': 1460,\n",
       " 'previous': 3400,\n",
       " 'weekend': 4797,\n",
       " 'sea': 3808,\n",
       " 'salt': 3754,\n",
       " 'water': 4771,\n",
       " 'desert': 1075,\n",
       " 'anyhow': 169,\n",
       " 'separate': 3863,\n",
       " 'rooms': 3710,\n",
       " 'room': 3708,\n",
       " 'monday': 2831,\n",
       " 'opposite': 3114,\n",
       " 'rarely': 3515,\n",
       " 'discussion': 1141,\n",
       " 'upon': 4638,\n",
       " 'topic': 4512,\n",
       " 'mistake': 2815,\n",
       " 'kinds': 2325,\n",
       " 'questions': 3486,\n",
       " 'negative': 2945,\n",
       " 'explained': 1416,\n",
       " 'pin': 3277,\n",
       " 'laughing': 2420,\n",
       " 'walked': 4727,\n",
       " 'figure': 1523,\n",
       " 'took': 4507,\n",
       " 'remove': 3619,\n",
       " 'foot': 1599,\n",
       " 'four years': 1629,\n",
       " 'years old': 4972,\n",
       " 'could say': 904,\n",
       " 'years later': 4971,\n",
       " 'first thing': 1558,\n",
       " 'http www': 2074,\n",
       " 'know much': 2351,\n",
       " 'men women': 2748,\n",
       " 'nothing else': 3007,\n",
       " 'longer': 2582,\n",
       " 'law': 2427,\n",
       " 'cross': 952,\n",
       " 'trick': 4554,\n",
       " 'broken': 520,\n",
       " 'san': 3757,\n",
       " 'diego': 1109,\n",
       " 'forget': 1609,\n",
       " 'sorry': 4058,\n",
       " 'mommy': 2829,\n",
       " 'brain': 494,\n",
       " 'seen': 3844,\n",
       " 'fox': 1631,\n",
       " 'rick': 3669,\n",
       " 'greg': 1875,\n",
       " 'thursday': 4442,\n",
       " 'naked': 2892,\n",
       " 'studio': 4213,\n",
       " 'guys': 1906,\n",
       " 'finding': 1541,\n",
       " 'air': 87,\n",
       " 'wet': 4827,\n",
       " 'pants': 3160,\n",
       " 'setting': 3878,\n",
       " 'record': 3581,\n",
       " 'pat': 3193,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_ #displaying the vocabulary from the count vectorizer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9e62b",
   "metadata": {
    "id": "b0d9e62b"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">3 D - Build a base model for Supervised Learning - Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "101581aa",
   "metadata": {
    "id": "101581aa"
   },
   "outputs": [],
   "source": [
    "#creating the instance of MultinomialNB model with alpha parameter \n",
    "model=MultinomialNB(alpha=.01)\n",
    "model.fit(X_train,ytrain) # fit the training data in the model instance\n",
    "ypred = model.predict(X_test) #making prediction from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d9010",
   "metadata": {
    "id": "9c6d9010"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">3 E - Clearly print Performance Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdf69543",
   "metadata": {
    "id": "fdf69543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:   0.297\n",
      "Precision Score:   0.216\n",
      "Recall Score:   0.297\n"
     ]
    }
   ],
   "source": [
    "#Performance metrics\n",
    "score = accuracy_score(ytest, ypred) #calculating accuracy score \n",
    "precision = precision_score(ytest, ypred,average='weighted') #calculating precision score\n",
    "recall = recall_score(ytest,ypred,average='weighted') #calculating recall score\n",
    "#printing the performance metrics\n",
    "print(\"Accuracy Score:   %0.3f\" % score)\n",
    "print(\"Precision Score:   %0.3f\" % precision)\n",
    "print(\"Recall Score:   %0.3f\" % recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abfc58",
   "metadata": {
    "id": "06abfc58"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">4 A - Experiment with other vectorisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cb1d90e",
   "metadata": {
    "id": "7cb1d90e"
   },
   "outputs": [],
   "source": [
    "#creating instance for TF-IDF vectorizer with parameters\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(1,4))\n",
    "X_tf_idf = tf_idf_vectorizer.fit_transform(X).toarray() #converting the text data X into numeric using the TF-IDF and making into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "298b33c7",
   "metadata": {
    "id": "298b33c7"
   },
   "outputs": [],
   "source": [
    "Xtrain_tf_idf,Xtest_tf_idf,ytrain,ytest = train_test_split(X_tf_idf,y,test_size=0.3,random_state=12) #creating the training and testing dataset using train_test_split function from X_tf_idf & y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd69f535",
   "metadata": {
    "id": "fd69f535"
   },
   "outputs": [],
   "source": [
    "model.fit(Xtrain_tf_idf,ytrain) # fit the training data in the base model \n",
    "ypred = model.predict(Xtest_tf_idf) #making prediction from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9c7e674",
   "metadata": {
    "id": "d9c7e674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:   0.388\n",
      "Precision Score:   0.366\n",
      "Recall Score:   0.388\n"
     ]
    }
   ],
   "source": [
    "#Performance metrics\n",
    "score = accuracy_score(ytest, ypred) #calculating accuracy score \n",
    "precision = precision_score(ytest, ypred,average='weighted') #calculating precision score\n",
    "recall = recall_score(ytest,ypred,average='weighted')#calculating recall score\n",
    "#printing the performance metrics\n",
    "print(\"Accuracy Score:   %0.3f\" % score)\n",
    "print(\"Precision Score:   %0.3f\" % precision)\n",
    "print(\"Recall Score:   %0.3f\" % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb257c7b",
   "metadata": {
    "id": "cb257c7b"
   },
   "outputs": [],
   "source": [
    "# Applying Word2 vec Wordembedding\n",
    "#perparing data for word Embedding\n",
    "words_list = []\n",
    "for i in data['Clean Text']:\n",
    "    li = list(i.split(\" \"))\n",
    "    words_list.append(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97cb2fd9",
   "metadata": {
    "id": "97cb2fd9"
   },
   "outputs": [],
   "source": [
    "# Model creation   \n",
    "model= Word2Vec(words_list, min_count = 1, workers = 4)\n",
    "# saving the model\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7165bc1",
   "metadata": {
    "id": "e7165bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104838\n"
     ]
    }
   ],
   "source": [
    "words = model.wv.index_to_key #getting the values from the model index to key function to words variable\n",
    "print(len(words)) #printing length of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47d798a5",
   "metadata": {
    "id": "47d798a5"
   },
   "outputs": [],
   "source": [
    "wvsd = model.wv[words] #getting model  words indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b31f7c44",
   "metadata": {
    "id": "b31f7c44"
   },
   "outputs": [],
   "source": [
    "#function for creating word embedding\n",
    "#defining function with word,model,vocabulary and num of features\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\") #generting np zero array with the length of the number of features\n",
    "    nwords = 0. #nwords variable with 0\n",
    "    #for loop from the words\n",
    "    for word in words:\n",
    "        if word in vocabulary: #checking for word in the vocabulary\n",
    "            nwords = nwords + 1. #add nword into 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word]) #add feature vector value and word index into the feature vector zero array \n",
    "    \n",
    "    if nwords: #checking for nwords values\n",
    "        feature_vector = np.divide(feature_vector, nwords) #dividing the feature vector \n",
    "        \n",
    "    return feature_vector # returing the feature vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features): #call the function\n",
    "    \n",
    "    vocabulary = set(model.wv.index_to_key) #making a set name vocabulary from words from the model\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in corpus] #making a list from corpus token sentences from the average word vector function\n",
    "    \n",
    "    return np.array(features) #returing the features as np array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b2fb7de",
   "metadata": {
    "id": "5b2fb7de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.273184</td>\n",
       "      <td>0.098726</td>\n",
       "      <td>0.280400</td>\n",
       "      <td>0.183860</td>\n",
       "      <td>-0.208212</td>\n",
       "      <td>-0.760461</td>\n",
       "      <td>0.449373</td>\n",
       "      <td>0.585213</td>\n",
       "      <td>-0.156152</td>\n",
       "      <td>-0.579903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596796</td>\n",
       "      <td>0.059671</td>\n",
       "      <td>-0.354892</td>\n",
       "      <td>0.135558</td>\n",
       "      <td>0.376102</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>0.159273</td>\n",
       "      <td>-0.316944</td>\n",
       "      <td>-0.243655</td>\n",
       "      <td>-0.050085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.285943</td>\n",
       "      <td>0.072359</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>-0.095572</td>\n",
       "      <td>-0.305558</td>\n",
       "      <td>-1.026383</td>\n",
       "      <td>0.642953</td>\n",
       "      <td>0.570524</td>\n",
       "      <td>-0.263432</td>\n",
       "      <td>-1.126519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704894</td>\n",
       "      <td>0.132940</td>\n",
       "      <td>-0.695402</td>\n",
       "      <td>0.452642</td>\n",
       "      <td>0.429551</td>\n",
       "      <td>0.310148</td>\n",
       "      <td>-0.114320</td>\n",
       "      <td>-0.480260</td>\n",
       "      <td>-0.271993</td>\n",
       "      <td>-0.240440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.063803</td>\n",
       "      <td>0.336951</td>\n",
       "      <td>-0.130620</td>\n",
       "      <td>0.613464</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>-1.157574</td>\n",
       "      <td>0.204765</td>\n",
       "      <td>0.627546</td>\n",
       "      <td>-0.769671</td>\n",
       "      <td>-0.762810</td>\n",
       "      <td>...</td>\n",
       "      <td>2.378349</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>-0.976431</td>\n",
       "      <td>0.352491</td>\n",
       "      <td>-0.982168</td>\n",
       "      <td>-0.154208</td>\n",
       "      <td>-0.279138</td>\n",
       "      <td>-0.478174</td>\n",
       "      <td>-0.170591</td>\n",
       "      <td>-0.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.327360</td>\n",
       "      <td>0.412342</td>\n",
       "      <td>0.492439</td>\n",
       "      <td>0.459046</td>\n",
       "      <td>-0.349096</td>\n",
       "      <td>-1.045358</td>\n",
       "      <td>0.598068</td>\n",
       "      <td>0.602835</td>\n",
       "      <td>-0.154824</td>\n",
       "      <td>-0.886743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895395</td>\n",
       "      <td>0.026481</td>\n",
       "      <td>-0.680626</td>\n",
       "      <td>0.051992</td>\n",
       "      <td>0.219147</td>\n",
       "      <td>0.589273</td>\n",
       "      <td>-0.101161</td>\n",
       "      <td>-0.078259</td>\n",
       "      <td>-0.195583</td>\n",
       "      <td>0.036364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.418884</td>\n",
       "      <td>0.190261</td>\n",
       "      <td>0.516711</td>\n",
       "      <td>0.060653</td>\n",
       "      <td>-1.040180</td>\n",
       "      <td>-0.887904</td>\n",
       "      <td>0.852759</td>\n",
       "      <td>0.507574</td>\n",
       "      <td>-0.209439</td>\n",
       "      <td>-0.979611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875724</td>\n",
       "      <td>0.082292</td>\n",
       "      <td>-0.547058</td>\n",
       "      <td>-0.121155</td>\n",
       "      <td>0.777290</td>\n",
       "      <td>1.051435</td>\n",
       "      <td>0.211437</td>\n",
       "      <td>-0.307468</td>\n",
       "      <td>-0.102203</td>\n",
       "      <td>-0.484032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28644</th>\n",
       "      <td>-1.279000</td>\n",
       "      <td>0.042484</td>\n",
       "      <td>-0.324437</td>\n",
       "      <td>0.379307</td>\n",
       "      <td>0.471290</td>\n",
       "      <td>-1.083894</td>\n",
       "      <td>0.023564</td>\n",
       "      <td>0.795872</td>\n",
       "      <td>0.287041</td>\n",
       "      <td>-0.927514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.528844</td>\n",
       "      <td>0.208263</td>\n",
       "      <td>-0.148910</td>\n",
       "      <td>0.012280</td>\n",
       "      <td>0.222036</td>\n",
       "      <td>1.162293</td>\n",
       "      <td>-0.110773</td>\n",
       "      <td>-0.518668</td>\n",
       "      <td>-0.849841</td>\n",
       "      <td>-0.067744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28645</th>\n",
       "      <td>-0.480383</td>\n",
       "      <td>0.394851</td>\n",
       "      <td>0.609007</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>0.339788</td>\n",
       "      <td>-0.383633</td>\n",
       "      <td>0.123862</td>\n",
       "      <td>0.777806</td>\n",
       "      <td>-0.090629</td>\n",
       "      <td>-0.628620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356375</td>\n",
       "      <td>-0.061860</td>\n",
       "      <td>-0.549273</td>\n",
       "      <td>-0.033639</td>\n",
       "      <td>0.818167</td>\n",
       "      <td>0.504199</td>\n",
       "      <td>0.130275</td>\n",
       "      <td>-0.399541</td>\n",
       "      <td>0.327082</td>\n",
       "      <td>-0.354515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28646</th>\n",
       "      <td>-0.318632</td>\n",
       "      <td>-0.158180</td>\n",
       "      <td>0.196987</td>\n",
       "      <td>-0.039179</td>\n",
       "      <td>-0.195626</td>\n",
       "      <td>-0.664604</td>\n",
       "      <td>0.556909</td>\n",
       "      <td>0.367798</td>\n",
       "      <td>-0.354340</td>\n",
       "      <td>-0.625903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779261</td>\n",
       "      <td>-0.003109</td>\n",
       "      <td>-0.468815</td>\n",
       "      <td>0.199878</td>\n",
       "      <td>0.200744</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>0.059738</td>\n",
       "      <td>-0.587869</td>\n",
       "      <td>-0.062424</td>\n",
       "      <td>-0.116664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28647</th>\n",
       "      <td>-0.180786</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>0.468319</td>\n",
       "      <td>0.182842</td>\n",
       "      <td>-0.987062</td>\n",
       "      <td>-1.087828</td>\n",
       "      <td>0.879041</td>\n",
       "      <td>0.546562</td>\n",
       "      <td>-0.237751</td>\n",
       "      <td>-1.116019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859150</td>\n",
       "      <td>-0.253775</td>\n",
       "      <td>-0.929214</td>\n",
       "      <td>0.167125</td>\n",
       "      <td>0.221436</td>\n",
       "      <td>0.221296</td>\n",
       "      <td>-0.096573</td>\n",
       "      <td>-0.586891</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.023362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28648</th>\n",
       "      <td>-0.239716</td>\n",
       "      <td>0.380640</td>\n",
       "      <td>0.250594</td>\n",
       "      <td>0.279742</td>\n",
       "      <td>-0.064351</td>\n",
       "      <td>-1.526633</td>\n",
       "      <td>0.869263</td>\n",
       "      <td>0.870475</td>\n",
       "      <td>-0.507345</td>\n",
       "      <td>-0.916852</td>\n",
       "      <td>...</td>\n",
       "      <td>1.175530</td>\n",
       "      <td>-0.013515</td>\n",
       "      <td>-0.799272</td>\n",
       "      <td>0.081495</td>\n",
       "      <td>-0.221742</td>\n",
       "      <td>0.414741</td>\n",
       "      <td>-0.472018</td>\n",
       "      <td>-0.344670</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>0.069537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28649 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.273184  0.098726  0.280400  0.183860 -0.208212 -0.760461  0.449373   \n",
       "1     -0.285943  0.072359  0.450923 -0.095572 -0.305558 -1.026383  0.642953   \n",
       "2     -0.063803  0.336951 -0.130620  0.613464  0.094118 -1.157574  0.204765   \n",
       "3     -0.327360  0.412342  0.492439  0.459046 -0.349096 -1.045358  0.598068   \n",
       "4     -0.418884  0.190261  0.516711  0.060653 -1.040180 -0.887904  0.852759   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "28644 -1.279000  0.042484 -0.324437  0.379307  0.471290 -1.083894  0.023564   \n",
       "28645 -0.480383  0.394851  0.609007 -0.063965  0.339788 -0.383633  0.123862   \n",
       "28646 -0.318632 -0.158180  0.196987 -0.039179 -0.195626 -0.664604  0.556909   \n",
       "28647 -0.180786  0.149658  0.468319  0.182842 -0.987062 -1.087828  0.879041   \n",
       "28648 -0.239716  0.380640  0.250594  0.279742 -0.064351 -1.526633  0.869263   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      0.585213 -0.156152 -0.579903  ...  0.596796  0.059671 -0.354892   \n",
       "1      0.570524 -0.263432 -1.126519  ...  0.704894  0.132940 -0.695402   \n",
       "2      0.627546 -0.769671 -0.762810  ...  2.378349  0.497056 -0.976431   \n",
       "3      0.602835 -0.154824 -0.886743  ...  0.895395  0.026481 -0.680626   \n",
       "4      0.507574 -0.209439 -0.979611  ...  0.875724  0.082292 -0.547058   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "28644  0.795872  0.287041 -0.927514  ...  1.528844  0.208263 -0.148910   \n",
       "28645  0.777806 -0.090629 -0.628620  ...  0.356375 -0.061860 -0.549273   \n",
       "28646  0.367798 -0.354340 -0.625903  ...  0.779261 -0.003109 -0.468815   \n",
       "28647  0.546562 -0.237751 -1.116019  ...  0.859150 -0.253775 -0.929214   \n",
       "28648  0.870475 -0.507345 -0.916852  ...  1.175530 -0.013515 -0.799272   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.135558  0.376102  0.074805  0.159273 -0.316944 -0.243655 -0.050085  \n",
       "1      0.452642  0.429551  0.310148 -0.114320 -0.480260 -0.271993 -0.240440  \n",
       "2      0.352491 -0.982168 -0.154208 -0.279138 -0.478174 -0.170591 -0.066500  \n",
       "3      0.051992  0.219147  0.589273 -0.101161 -0.078259 -0.195583  0.036364  \n",
       "4     -0.121155  0.777290  1.051435  0.211437 -0.307468 -0.102203 -0.484032  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "28644  0.012280  0.222036  1.162293 -0.110773 -0.518668 -0.849841 -0.067744  \n",
       "28645 -0.033639  0.818167  0.504199  0.130275 -0.399541  0.327082 -0.354515  \n",
       "28646  0.199878  0.200744  0.016839  0.059738 -0.587869 -0.062424 -0.116664  \n",
       "28647  0.167125  0.221436  0.221296 -0.096573 -0.586891 -0.095332 -0.023362  \n",
       "28648  0.081495 -0.221742  0.414741 -0.472018 -0.344670  0.030806  0.069537  \n",
       "\n",
       "[28649 rows x 100 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_size = 100 #defining the feature size\n",
    "\n",
    "# get document level embeddings\n",
    "# averaged_word_vectorizer and making w2v_feature_array\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus = words_list, model = model,num_features = feature_size)\n",
    "pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfedde6a",
   "metadata": {
    "id": "bfedde6a"
   },
   "outputs": [],
   "source": [
    "Xtrain_w2v,Xtest_w2v,ytrain,ytest = train_test_split(w2v_feature_array,y, test_size=0.3, random_state=12) #creating the training and testing dataset using train_test_split function from w2v_feature_array & y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c835ada",
   "metadata": {
    "id": "5c835ada"
   },
   "outputs": [],
   "source": [
    "#creating a instance for MinMaxScaler \n",
    "scaler = MinMaxScaler()\n",
    "Xtrain_w2v = scaler.fit_transform(Xtrain_w2v) #fitand Transforming the Xtrain data\n",
    "Xtest_w2v = scaler.fit_transform(Xtest_w2v) #fir and Transforming the Xtest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1920c6d",
   "metadata": {
    "id": "c1920c6d"
   },
   "outputs": [],
   "source": [
    "#creating instance for the MultinomialNB base mode\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain_w2v,ytrain) #fit the training data into model\n",
    "ypred = model.predict(Xtest_w2v) #making prediction from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41f5177e",
   "metadata": {
    "id": "41f5177e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:   0.367\n",
      "Precision Score:   0.134\n",
      "Recall Score:   0.367\n"
     ]
    }
   ],
   "source": [
    "#Performance metrics\n",
    "score = accuracy_score(ytest, ypred) #calculating accuracy score \n",
    "precision = precision_score(ytest, ypred,average='weighted') #calculating precision score\n",
    "recall = recall_score(ytest,ypred,average='weighted') #calculating recall score\n",
    "#printing the performance metrics\n",
    "print(\"Accuracy Score:   %0.3f\" % score)\n",
    "print(\"Precision Score:   %0.3f\" % precision)\n",
    "print(\"Recall Score:   %0.3f\" % recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682cf7f",
   "metadata": {
    "id": "7682cf7f"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">4 B - Build classifier Models using other algorithms than base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b99f1634",
   "metadata": {
    "id": "b99f1634"
   },
   "outputs": [],
   "source": [
    "#making pipeline by creating list with model name and instances\n",
    "model_pipeline = []\n",
    "model_pipeline.append(['Logistic Regression',LogisticRegression()]) #appending the logistic regression\n",
    "model_pipeline.append(['KNN',KNeighborsClassifier()])  #appending the KNeighborsClassifier\n",
    "model_pipeline.append(['Decision Tree Classifier',DecisionTreeClassifier()])  #appending the Decision Tree Classifier\n",
    "model_pipeline.append(['Gaussian',GaussianNB()]) # #appending the GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82bbaaa",
   "metadata": {
    "id": "d82bbaaa"
   },
   "source": [
    "Buliding Classifier Model for Count Vectorizer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "984319ce",
   "metadata": {
    "id": "984319ce"
   },
   "outputs": [],
   "source": [
    "#model Evaluation\n",
    "model_eval = [] #empty list to store the meterics for the pipeline\n",
    "\n",
    "for model in model_pipeline: #for loop thru the model list\n",
    "    value = [] #empty list\n",
    "    model[1].fit(X_train,ytrain) #fiting traing data into the selected model\n",
    "    ypred = model[1].predict(X_test) # model preodction\n",
    "    #perfromace metrics\n",
    "    acc_score = accuracy_score(ypred,ytest)\n",
    "    pre_score = precision_score(ytest, ypred,average='weighted')\n",
    "    rel_score = recall_score(ytest,ypred,average='weighted')\n",
    "    value.append(model[0])\n",
    "    value.append(acc_score)\n",
    "    value.append(pre_score)\n",
    "    value.append(rel_score)\n",
    "    model_eval.append(value)\n",
    "result_with_count = pd.DataFrame(model_eval,columns=['Model','Accuracy Score','Precision Score','Recall Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c8388",
   "metadata": {
    "id": "914c8388"
   },
   "source": [
    "Buliding Classifier Model for TF-IDF Vectorizer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00e03a23",
   "metadata": {
    "id": "00e03a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 46s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model Evaluation\n",
    "model_eval = [] #empty list to store the meterics for the pipeline\n",
    "\n",
    "for model in model_pipeline: #for loop thru the model list\n",
    "    value = [] #empty list\n",
    "    model[1].fit(Xtrain_tf_idf,ytrain) #fiting traing data into the selected model\n",
    "    ypred = model[1].predict(Xtest_tf_idf) # model preodction\n",
    "    #perfromace metrics\n",
    "    acc_score = accuracy_score(ypred,ytest)\n",
    "    pre_score = precision_score(ytest, ypred,average='weighted')\n",
    "    rel_score = recall_score(ytest,ypred,average='weighted')\n",
    "    value.append(model[0])\n",
    "    value.append(acc_score)\n",
    "    value.append(pre_score)\n",
    "    value.append(rel_score)\n",
    "    model_eval.append(value)\n",
    "result_with_tfidf = pd.DataFrame(model_eval,columns=['Model','Accuracy Score','Precision Score','Recall Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee0987",
   "metadata": {
    "id": "09ee0987"
   },
   "source": [
    "Buliding Classifier Model for Word2vec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8af7401e",
   "metadata": {
    "id": "8af7401e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.8 s\n",
      "Wall time: 7.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model Evaluation\n",
    "model_eval = [] #empty list to store the meterics for the pipeline\n",
    "\n",
    "for model in model_pipeline:\n",
    "    value = [] #empty list\n",
    "    model[1].fit(Xtrain_w2v,ytrain) #fiting traing data into the selected model\n",
    "    ypred = model[1].predict(Xtest_w2v) # model preodction\n",
    "    #perfromace metrics\n",
    "    acc_score = accuracy_score(ypred,ytest)\n",
    "    pre_score = precision_score(ytest, ypred,average='weighted')\n",
    "    rel_score = recall_score(ytest,ypred,average='weighted')\n",
    "    value.append(model[0])\n",
    "    value.append(acc_score)\n",
    "    value.append(pre_score)\n",
    "    value.append(rel_score)\n",
    "    model_eval.append(value)\n",
    "result_with_wv = pd.DataFrame(model_eval,columns=['Model','Accuracy Score','Precision Score','Recall Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88385ba",
   "metadata": {
    "id": "b88385ba"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">4 D - Clearly print Performance Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9297690d",
   "metadata": {
    "id": "9297690d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>Recall Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.232344</td>\n",
       "      <td>0.202090</td>\n",
       "      <td>0.232344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.252356</td>\n",
       "      <td>0.198510</td>\n",
       "      <td>0.252356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.223153</td>\n",
       "      <td>0.205148</td>\n",
       "      <td>0.223153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.209308</td>\n",
       "      <td>0.201089</td>\n",
       "      <td>0.209308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy Score  Precision Score  Recall Score\n",
       "0       Logistic Regression        0.232344         0.202090      0.232344\n",
       "1                       KNN        0.252356         0.198510      0.252356\n",
       "2  Decision Tree Classifier        0.223153         0.205148      0.223153\n",
       "3                  Gaussian        0.209308         0.201089      0.209308"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_count #displaying the performace of the various models on count vectorizer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce4258be",
   "metadata": {
    "id": "ce4258be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>Recall Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.381035</td>\n",
       "      <td>0.275392</td>\n",
       "      <td>0.381035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.363467</td>\n",
       "      <td>0.252387</td>\n",
       "      <td>0.363467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.251774</td>\n",
       "      <td>0.225601</td>\n",
       "      <td>0.251774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.131937</td>\n",
       "      <td>0.241478</td>\n",
       "      <td>0.131937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy Score  Precision Score  Recall Score\n",
       "0       Logistic Regression        0.381035         0.275392      0.381035\n",
       "1                       KNN        0.363467         0.252387      0.363467\n",
       "2  Decision Tree Classifier        0.251774         0.225601      0.251774\n",
       "3                  Gaussian        0.131937         0.241478      0.131937"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_tfidf #displaying the performace of the various models on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b919c49",
   "metadata": {
    "id": "0b919c49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>Recall Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.368121</td>\n",
       "      <td>0.241623</td>\n",
       "      <td>0.368121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.260733</td>\n",
       "      <td>0.224996</td>\n",
       "      <td>0.260733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.178709</td>\n",
       "      <td>0.201388</td>\n",
       "      <td>0.178709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.020361</td>\n",
       "      <td>0.217520</td>\n",
       "      <td>0.020361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy Score  Precision Score  Recall Score\n",
       "0       Logistic Regression        0.368121         0.241623      0.368121\n",
       "1                       KNN        0.260733         0.224996      0.260733\n",
       "2  Decision Tree Classifier        0.178709         0.201388      0.178709\n",
       "3                  Gaussian        0.020361         0.217520      0.020361"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_wv ##displaying the performace of the various models on Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a42500",
   "metadata": {
    "id": "48a42500"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">5 A - Which vectorizer performed better? Probable reason?."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aba390",
   "metadata": {
    "id": "b0aba390"
   },
   "source": [
    "## TF-IDF(term frequency-inverse document frequency)\n",
    "\n",
    "### From the above evaluation the the TF-IDF(term frequency-inverse document frequency) vectorization give higher accuracy score compare to all the other vectorization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31943da5",
   "metadata": {
    "id": "31943da5"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">5 B - Which model outperformed? Probable reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92301b1",
   "metadata": {
    "id": "e92301b1"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### From the above model comparision the Logistic Refression give higher accuracy score with the TF-IDF data vectorization method gives 38.5 accuray score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cc0f2",
   "metadata": {
    "id": "486cc0f2"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">5 C - Which parameter/hyperparameter significantly helped to improve performance? Probable reason?."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9133d",
   "metadata": {
    "id": "95d9133d"
   },
   "source": [
    "### Vectorization method play major role affecting the perfomance of the model, using different vectorization methods with base model give base average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633f81c",
   "metadata": {
    "id": "2633f81c"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">5 D - According to you, which performance metric should be given most importance, why?."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5a36e",
   "metadata": {
    "id": "56d5a36e"
   },
   "source": [
    "### ROC AUC \n",
    "ROC curves are widely used to compare and evaluate different classification algorithms.\n",
    "\n",
    "ROC curve is widely used when the dataset is imbalanced.\n",
    "\n",
    "ROC curves are also used in verification of forecasts in meteorology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f185e",
   "metadata": {
    "id": "db8f185e"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:2.5em;color:#b44b97;\"> Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7ecab",
   "metadata": {
    "id": "3dc7ecab"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3A8D71;\">DOMAIN:</span> Customer support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c81528",
   "metadata": {
    "id": "b8c81528"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3A8D71;\">CONTENT:</span> Great Learning has a an academic support department which receives numerous support requests every day throughout the year.\n",
    "Teams are spread across geographies and try to provide support round the year. Sometimes there are circumstances where due to heavy\n",
    "workload certain request resolutions are delayed, impacting company’s business. Some of the requests are very generic where a proper\n",
    "resolution procedure delivered to the user can solve the problem. Company is looking forward to design an automation which can interact with\n",
    "the user, understand the problem and display the resolution procedure [ if found as a generic request ] or redirect the request to an actual human\n",
    "support executive if the request is complex or not in it’s database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1686cc",
   "metadata": {
    "id": "af1686cc"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3A8D71;\">DATA DESCRIPTION:</span> \n",
    "A sample corpus is attached for your reference. Please enhance/add more data to the corpus using your linguistics skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5538f3",
   "metadata": {
    "id": "be5538f3"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1em;color:#3A8D71;\">PROJECT OBJECTIVE:</span> Design a python based interactive semi - rule based chatbot which can do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdd5cc",
   "metadata": {
    "id": "eafdd5cc"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">1 - Start chat session with greetings and ask what the user is looking for.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6adb4",
   "metadata": {
    "id": "5ff6adb4"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">2 - Accept dynamic text based questions from the user. Reply back with relevant answer from the designed corpus.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b89fd4",
   "metadata": {
    "id": "55b89fd4"
   },
   "source": [
    "<span style=\"font-family:Ebrima; font-weight:bold;font-size:1.5em;color:#b44b97;\">3 - End the chat session only if the user requests to end else ask what the user is looking for. Loop continues till the user asks to end it.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11458ce",
   "metadata": {
    "id": "f11458ce"
   },
   "source": [
    "# Chatbot Buliding\n",
    "\n",
    "1. Running a Chatbot \"greet the user and ask what the user looking for\"\n",
    "2. Dynamic textbased\n",
    "    For Dynamic textproecssing need to build the model that can process the complex text\n",
    "    1. Text Case - Handling upper and lower case\n",
    "    \n",
    "    ## Text preprocessing\n",
    "    \n",
    "    2. tokenization\n",
    "    \n",
    "    3. stemming\n",
    "    \n",
    "    4. bag of words (BoW)\n",
    "    \n",
    "    5. Text Classification model buliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d03a60f0",
   "metadata": {
    "id": "d03a60f0"
   },
   "outputs": [],
   "source": [
    "#importing necessary python libraries\n",
    "import nltk\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "from nltk.chat.util import Chat,reflections\n",
    "\n",
    "#importing pickle for pickling the models\n",
    "import pickle\n",
    "\n",
    "#import load model \n",
    "from keras.models import load_model\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5a2054a",
   "metadata": {
    "id": "e5a2054a"
   },
   "outputs": [],
   "source": [
    "#creating pairs and reflections for the nltk chat function\n",
    "pairs = [\n",
    "[\n",
    "    r\"hello\",\n",
    "    [\"Hello %1, How are you today ?\",]\n",
    "],\n",
    "[\n",
    "    r\"my name is (.*)\",\n",
    "    [\"Hello %1, How are you today ?\",]\n",
    "],\n",
    "[\n",
    "    r\"what is your name ?\",\n",
    "    [\"My name is Chatbot and I'm a chatbot ?\",]\n",
    "],\n",
    "[\n",
    "    r\"how are you ?\",\n",
    "    [\"I'm doing good\\nHow about You ?\",]\n",
    "],\n",
    "[\n",
    "    r\"what (.*) want ?\",\n",
    "    [\"Make me an offer I can't refuse\",]       \n",
    "],\n",
    "\n",
    "]\n",
    "\n",
    "reflections= {\n",
    " \"go\"     : \"gone\",\n",
    " \"hi\"    : \"hey there\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf7088ec",
   "metadata": {
    "id": "bf7088ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm Virual Assistance a chatbot;Please type lowercase English language to start a conversation. Type quit to leave \n",
      ">my Name is Ranjith\n",
      "Hello ranjith, How are you today ?\n",
      ">quit\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#defining function name chat bot with introduction and get user input and response  \n",
    "def chatbot():\n",
    "    #chatbot intro\n",
    "    print(\"Hi, I'm Virual Assistance a chatbot;Please type lowercase English language to start a conversation. Type quit to leave \") #default message at the start\n",
    "    #chat function with probem\n",
    "    chat = Chat(pairs, reflections) \n",
    "    chat.converse()\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot() #callig the chat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5db8445",
   "metadata": {
    "id": "f5db8445"
   },
   "outputs": [],
   "source": [
    "#loading Json file as corpus\n",
    "data_file=open('GL Bot.json').read() #reading the GL Bot json file as datafile \n",
    "intents=json.loads(data_file) #loading the  into intents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c0bd168",
   "metadata": {
    "id": "2c0bd168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'Intro', 'patterns': ['hi', 'how are you', 'is anyone there', 'hello', 'whats up', 'hey', 'yo', 'listen', 'please help me', 'i am learner from', 'i belong to', 'aiml batch', 'aifl batch', 'i am from', 'my pm is', 'blended', 'online', 'i am from', 'hey ya', 'talking to you for first time'], 'responses': ['Hello! how can i help you ?'], 'context_set': ''}, {'tag': 'Exit', 'patterns': ['thank you', 'thanks', 'cya', 'see you', 'later', 'see you later', 'goodbye', 'i am leaving', 'have a Good day', 'you helped me', 'thanks a lot', 'thanks a ton', 'you are the best', 'great help', 'too good', 'you are a good learning buddy'], 'responses': ['I hope I was able to assist you, Good Bye'], 'context_set': ''}, {'tag': 'Olympus', 'patterns': ['olympus', 'explain me how olympus works', 'I am not able to understand olympus', 'olympus window not working', 'no access to olympus', 'unable to see link in olympus', 'no link visible on olympus', 'whom to contact for olympus', 'lot of problem with olympus', 'olypus is not a good tool', 'lot of problems with olympus', 'how to use olympus', 'teach me olympus'], 'responses': ['Link: Olympus wiki'], 'context_set': ''}, {'tag': 'SL', 'patterns': ['i am not able to understand svm', 'explain me how machine learning works', 'i am not able to understand naive bayes', 'i am not able to understand logistic regression', 'i am not able to understand ensemble techb=niques', 'i am not able to understand knn', 'i am not able to understand knn imputer', 'i am not able to understand cross validation', 'i am not able to understand boosting', 'i am not able to understand random forest', 'i am not able to understand ada boosting', 'i am not able to understand gradient boosting', 'machine learning', 'ML', 'SL', 'supervised learning', 'knn', 'logistic regression', 'regression', 'classification', 'naive bayes', 'nb', 'ensemble techniques', 'bagging', 'boosting', 'ada boosting', 'ada', 'gradient boosting', 'hyper parameters'], 'responses': ['Link: Machine Learning wiki '], 'context_set': ''}, {'tag': 'NN', 'patterns': ['what is deep learning', 'unable to understand deep learning', 'explain me how deep learning works', 'i am not able to understand deep learning', 'not able to understand neural nets', 'very diffult to understand neural nets', 'unable to understand neural nets', 'ann', 'artificial intelligence', 'artificial neural networks', 'weights', 'activation function', 'hidden layers', 'softmax', 'sigmoid', 'relu', 'otimizer', 'forward propagation', 'backward propagation', 'epochs', 'epoch', 'what is an epoch', 'adam', 'sgd'], 'responses': ['Link: Neural Nets wiki'], 'context_set': ''}, {'tag': 'Bot', 'patterns': ['what is your name', 'who are you', 'name please', 'when are your hours of opertions', 'what are your working hours', 'hours of operation', 'working hours', 'hours'], 'responses': ['I am your virtual learning assistant'], 'context_set': ''}, {'tag': 'Profane', 'patterns': ['what the hell', 'bloody stupid bot', 'do you think you are very smart', 'screw you', 'i hate you', 'you are stupid', 'jerk', 'you are a joke', 'useless piece of shit'], 'responses': ['Please use respectful words'], 'context_set': ''}, {'tag': 'Ticket', 'patterns': ['my problem is not solved', 'you did not help me', 'not a good solution', 'bad solution', 'not good solution', 'no help', 'wasted my time', 'useless bot', 'create a ticket'], 'responses': ['Tarnsferring the request to your PM'], 'context_set': ''}, {'tag': 'hello', 'patterns': ['hello'], 'responses': ['hello there']}]}\n"
     ]
    }
   ],
   "source": [
    "#printing the intents\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bcbe4e81",
   "metadata": {
    "id": "bcbe4e81"
   },
   "outputs": [],
   "source": [
    "# Tokenization \n",
    "# empty list to store the values\n",
    "words=[]\n",
    "classes=[]\n",
    "documents=[]\n",
    "#ingore context list\n",
    "ignore=['?','!',',',\"'s\"]\n",
    "\n",
    "#for loop in intents-intents\n",
    "for intent in intents['intents']:\n",
    "  #for loop in intent- patterns\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize the pattern \n",
    "        w=nltk.word_tokenize(pattern)\n",
    "        words.extend(w) #extenb the words list\n",
    "        documents.append((w,intent['tag'])) #appending the W and intent tag \n",
    "        \n",
    "        if intent['tag'] not in classes: #if intente tag is not in the classes list\n",
    "            classes.append(intent['tag']) #add to the classes list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77590c75",
   "metadata": {
    "id": "77590c75"
   },
   "outputs": [],
   "source": [
    "#applying lemmatizer on the words list if word not in ignore list\n",
    "words=[lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore]\n",
    "words=sorted(list(set(words))) #making sorted list with set of words\n",
    "classes=sorted(list(set(classes))) #making sorted list with set of classes\n",
    "pickle.dump(words,open('words.pkl','wb')) #dumping the words list as pickle file\n",
    "pickle.dump(classes,open('classes.pkl','wb')) #dumping the classes list as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "205a91a8",
   "metadata": {
    "id": "205a91a8"
   },
   "outputs": [],
   "source": [
    "#generating training and testing data \n",
    "\n",
    "#function bag of words to generate the training and testing data\n",
    "def bag_of_words():\n",
    "    training=[] #empty list\n",
    "    output_empty=[0]*len(classes) #Zero list size of the classes file\n",
    "\n",
    "    for doc in documents: #creating for loop in the document\n",
    "        bag=[] #empty list\n",
    "        pattern=doc[0] #patten equal to the document 0th instance value\n",
    "        pattern=[ lemmatizer.lemmatize(word.lower()) for word in pattern ] #applying lemmatixation in the pattern list\n",
    "\n",
    "        for word in words: #for loop in words \n",
    "            if word in pattern: #if word in the pattern\n",
    "                bag.append(1) #making bag list entry into 1\n",
    "            else:\n",
    "                bag.append(0) #making bag list entry into 0\n",
    "        output_row=list(output_empty) #making output row by listing the output_empty list\n",
    "        output_row[classes.index(doc[1])]=1 #setting index to 1\n",
    "        training.append([bag,output_row]) #appeding the values into the traing data list\n",
    "\n",
    "    random.shuffle(training) #shuffle the traing list\n",
    "    training=np.array(training) #making into array \n",
    "    X_train=list(training[:,0]) # making the Xtrain list from the train array 0th entry\n",
    "    y_train=list(training[:,1]) # making the ytrain list from the train array 1th entry\n",
    "    \n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d5f4aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = bag_of_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6586071",
   "metadata": {
    "id": "b6586071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 1s 4ms/step - loss: 2.1888 - accuracy: 0.1163 \n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.1693 - accuracy: 0.1628\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1147 - accuracy: 0.2171\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0400 - accuracy: 0.2791\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0209 - accuracy: 0.2403\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9592 - accuracy: 0.2791\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.9004 - accuracy: 0.3256\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8388 - accuracy: 0.3178\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.7892 - accuracy: 0.3256\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.7599 - accuracy: 0.3643\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.6929 - accuracy: 0.4264\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5711 - accuracy: 0.4264\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.4489 - accuracy: 0.5349\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4218 - accuracy: 0.5504\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.2613 - accuracy: 0.5969\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.1506 - accuracy: 0.6667\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.0000 - accuracy: 0.7132\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.9154 - accuracy: 0.7597\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.9193 - accuracy: 0.6977\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.8032 - accuracy: 0.7829\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7174 - accuracy: 0.7829\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.8372\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5379 - accuracy: 0.8915\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 0.8837\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.8682\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4722 - accuracy: 0.9070\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3628 - accuracy: 0.8992\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.8992\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.9380\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.9147\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2810 - accuracy: 0.9302\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.9225\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.9612\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9690\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9457\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9535\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2095 - accuracy: 0.9535\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2110 - accuracy: 0.9612\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1748 - accuracy: 0.9845\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9612\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9612\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.9767\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0977 - accuracy: 0.9845\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1618 - accuracy: 0.9535\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9535\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.9845\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0933 - accuracy: 0.9767\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9535\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1479 - accuracy: 0.9612\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9302\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.9845\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1102 - accuracy: 0.9690\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0917 - accuracy: 0.9845\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1127 - accuracy: 0.9767\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9845\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9767\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.9690\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0760 - accuracy: 0.9767\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9767\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9767\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9845\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 762us/step - loss: 0.0660 - accuracy: 0.9767\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9690\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9767\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9767\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9845\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9845\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9767\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9922\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0519 - accuracy: 0.9922\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9922\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0519 - accuracy: 0.9922\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 0.9922\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0603 - accuracy: 0.9845\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9922\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.9922\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9845\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9845\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 0.9845\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0407 - accuracy: 0.9922\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0420 - accuracy: 0.9845\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9845\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0661 - accuracy: 0.9845\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.9922\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9922\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0271 - accuracy: 0.9922\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9845\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9690\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9845\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.9845\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9845\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0346 - accuracy: 0.9922\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9922\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.9690\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0339 - accuracy: 0.9845\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9845\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9845\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9845\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9767\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9922\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0338 - accuracy: 0.9845\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9845\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 0.9845\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 954us/step - loss: 0.0586 - accuracy: 0.9767\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9922\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9845\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9767\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0902 - accuracy: 0.9690\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9845\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 0.9845\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 929us/step - loss: 0.0345 - accuracy: 0.9845\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9845\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.9922\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9922\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.9845\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9922\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.9922\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.9922\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 0.9922\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.9922\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.9922\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0387 - accuracy: 0.9845\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9845\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9922\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9845\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.9922\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9922\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9922\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9845\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.9845\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9922\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.9922\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0284 - accuracy: 0.9922\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9767\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.9922\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0316 - accuracy: 0.9922\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9922\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9922\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.9845\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.9922\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.9845\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9922\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9845\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9845\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.9922\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9845\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9922\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9922\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9922\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9922\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 0.9845\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.9845\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9767\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9922\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0234 - accuracy: 0.9922\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9922\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9767\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9845\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9922\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.9922\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0176 - accuracy: 0.9922\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9845\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9922\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0265 - accuracy: 0.9922\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9845\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.9845\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 987us/step - loss: 0.0200 - accuracy: 0.9922\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 418us/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0227 - accuracy: 0.9845\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.9845\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9845\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.9845\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9845\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.9845\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9922\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0370 - accuracy: 0.9767\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9922\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9922\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9845\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.9845\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9922\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9922\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 779us/step - loss: 0.0170 - accuracy: 0.9922\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9922\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.9845\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.9922\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9922\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "#Buliding Model\n",
    "model=Sequential() #sequential model\n",
    "model.add(Dense(128,activation='relu',input_shape=(len(X_train[0]),))) #dense layer / input layer with length of the Xtrain rows\n",
    "model.add(Dropout(0.5)) #dropout layer\n",
    "model.add(Dense(64,activation='relu')) #hidden layer 1\n",
    "model.add(Dense(64,activation='relu')) #hidden Layer 2\n",
    "model.add(Dropout(0.5)) #dropout layer \n",
    "model.add(Dense(len(y_train[0]),activation='softmax')) #output layer with softmax activation function\n",
    "\n",
    "adam=tf.keras.optimizers.Adam(0.001) #defining the adam optimizer function with learing ratw\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy']) #compiling the model\n",
    "weights=model.fit(np.array(X_train),np.array(y_train),epochs=200,batch_size=10,verbose=1)    #fit the model with X train and y train with 200 epochs\n",
    "model.save('mymodel.h5',weights) #saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f95a656",
   "metadata": {
    "id": "0f95a656"
   },
   "outputs": [],
   "source": [
    "#loading model,json,words pickle and classes pickle\n",
    "model = load_model('mymodel.h5')\n",
    "intents = json.loads(open('GL Bot.json').read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "715KETDId3G_",
   "metadata": {
    "id": "715KETDId3G_"
   },
   "outputs": [],
   "source": [
    "#function for cleaning up the words from the user \n",
    "def clean_up(sentence):\n",
    "    sentence_words=nltk.word_tokenize(sentence) #nltk tokenizing the sentences\n",
    "    sentence_words=[ lemmatizer.lemmatize(word.lower()) for word in sentence_words] #apply lemmatization on the words\n",
    "    return sentence_words #returning the clean words as sentence words\n",
    "\n",
    "def create_bow(sentence,words): #creating bag of words function\n",
    "    sentence_words=clean_up(sentence) #calling the clean function with sentence and assign into the sentence_words\n",
    "    bag=list(np.zeros(len(words))) #making the list of zero array with size of the words for stroing the BOW\n",
    "    \n",
    "    for s in sentence_words: #for loop thru the sentence_words\n",
    "        for i,w in enumerate(words): #getting index and word\n",
    "            if w == s:  # word equal to sentence\n",
    "                bag[i] = 1 #make bag list indexed instance into 1\n",
    "    return np.array(bag) #return the bag of words list as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a2c5e7e",
   "metadata": {
    "id": "7a2c5e7e"
   },
   "outputs": [],
   "source": [
    "#prediction function\n",
    "def predict_class(sentence,model): #function taking model and sentence from user\n",
    "    p=create_bow(sentence,words) #calling the create bag of words with sentence and word\n",
    "    res=model.predict(np.array([p]))[0] # making prediction with P and its 0th entry\n",
    "    threshold=0.8 #defining the threshold \n",
    "    results=[[i,r] for i,r in enumerate(res) if r>threshold] #creating list with result of the model prediction\n",
    "    results.sort(key=lambda x: x[1],reverse=True) # sorting the result\n",
    "    \n",
    "    return_list=[] #empty list\n",
    "    for result in results: #for loop thru the result\n",
    "        return_list.append({'intent':classes[result[0]],'prob':str(result[1])}) #append the intent classes result and result to the queary into the returm list\n",
    "    return return_list #returing the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e9b30b2b",
   "metadata": {
    "id": "e9b30b2b"
   },
   "outputs": [],
   "source": [
    "#defing the response function\n",
    "def get_response(return_list,intents_json):\n",
    "  #giving the chatbot some basic tags for the extreme ondtion\n",
    "    if len(return_list)==0: #no retunlist \n",
    "        tag='noanswer' #print(no answer)\n",
    "    else:    \n",
    "        tag=return_list[0]['intent'] #else return the retrun list intent value\n",
    "        \n",
    "        list_of_intents= intents_json['intents']    #making list of intents for json intents\n",
    "    \n",
    "        for i in list_of_intents: #looping thru the list of intents\n",
    "            if tag==i['tag'] : #getting the tags\n",
    "                result = random.choice(i['responses']) #making the random respose & assign to the result\n",
    "    \n",
    "        return result #retuning the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c4fc538",
   "metadata": {
    "id": "8c4fc538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome Im Your Vitual Assitance, How May I Help you Today!\n",
      "Happy Learning!\n",
      "You:help me understand NLP\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "\n",
      "Bot: None\n",
      "You:i am not able to understand gradient boosting\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "\n",
      "Bot: Link: Machine Learning wiki \n",
      "You:ml\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "\n",
      "Bot: Link: Machine Learning wiki \n",
      "You:sl\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "Bot: Link: Machine Learning wiki \n",
      "You:i am not able to understand naive bayes\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\n",
      "Bot: Link: Machine Learning wiki \n",
      "You:my problem is not solved\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\n",
      "Bot: Tarnsferring the request to your PM\n",
      "You:not a good solution\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "\n",
      "Bot: Tarnsferring the request to your PM\n",
      "You:thanks a lot\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "Bot: I hope I was able to assist you, Good Bye\n",
      "You:quit\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "\n",
      "Bot: None\n",
      "Thank you, Happy Learning!\n"
     ]
    }
   ],
   "source": [
    "#defining the chatbot function respose\n",
    "def response(text):\n",
    "    return_list=predict_class(text,model) #calling the predict call function for making the prediction for the given text\n",
    "    response=get_response(return_list,intents) #calling the get response function for getting the intent and its response\n",
    "    return response #return the response\n",
    "print(\"Welcome Im Your Vitual Assitance, How May I Help you Today!\\nHappy Learning!\") #printing intro\n",
    "while(1): #loop runs continues\n",
    "    x=input(\"You:\") # getting user inpout\n",
    "    res = response(x) #call response\n",
    "    print(\"\\nBot:\",res) #print the result as bot response\n",
    "    if x.lower() in ['quit','bye','goodbye','get lost','see you']: # if x value is any of it in the list\n",
    "        print(\"Thank you, Happy Learning!\") #print comment\n",
    "        break #break the loop"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
